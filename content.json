{"meta":{"title":"徐乔伟","subtitle":"","description":"徐乔伟的个人博客","author":"徐乔伟","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2022-09-25T08:04:37.346Z","updated":"2022-09-25T08:04:37.346Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"下面写关于自己的内容"},{"title":"所有标签","date":"2022-09-25T08:17:53.053Z","updated":"2022-09-25T08:17:53.053Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2022-09-25T08:06:39.390Z","updated":"2022-09-25T08:06:39.390Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"layout: friends # 必须title: 我的朋友们 # 可选，这是友链页的标题 这里写友链上方的内容。","text":"layout: friends # 必须title: 我的朋友们 # 可选，这是友链页的标题 这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"所有分类","date":"2022-09-25T08:07:13.457Z","updated":"2022-09-25T08:07:13.457Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"从零实现Golang的HTTP标准库-（6）","slug":"从零实现Golang的HTTP标准库-（6）","date":"2022-10-09T00:20:00.000Z","updated":"2022-10-06T09:52:24.105Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（6）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%886%EF%BC%89.html","excerpt":"","text":"1.概述前文对Body进行了设置，不论客户端是使用Content-Type还是chunk编码的方式，服务端都能够正确的解析。本文将在Body功能的基础上，着重完成form表单中的multipart表单的解析。个人认为是整个框架最难实现部分，不过与chunkReader实现的思路类似，可以借鉴实现。 2.两种form表单客户端提交的form表单是利用http请求的报文主体部分携带的，前文已经做到了去轻松读取报文主体的字节数据，今天的任务就是从字节流中解析出表单内容。 按照RFC标准而言，POST请求存在两种表单： application&#x2F;x-www-form-urlencoded。只能用来提交纯文本请求参数，与url的queryString字段起一样的作用，例前端的用户名和密码一般通过这个表单传递给后端。 前端通过以下代码发起该form请求： 12345&lt;form action=&quot;/login&quot; method=&quot;post&quot; enctype=”application/x-www-form-urlencoded”&gt; name:&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; password:&lt;input type=&quot;text&quot; name=&quot;password&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;login&quot;&gt; &lt;/form&gt; 转化为的报文大致如下： 1234567POST /login HTTP/1.1\\r\\n# 通过Content-Type告知对端传输的是哪种表单Content-Type: application/x-www-form-urlencoded Content-Length: 20\\r\\n#other unconcerned headers\\r\\nname=gu&amp;password=123 #报文主体 form表单会出现在报文主体部分，以&#x3D;拼接key、value，以&amp;拼接每一项，与queryString如出一辙。其解析很简单，相信读者坚持到这里后能够自行完成解析，用一个map保存KV即可，我们在下一讲中简要介绍。 multipart&#x2F;form-data。这个表单的功能比上一个表单强大许多，不仅可以发送文本请求，还可以发送任意数量的文件，当然功能强大的同时会增加解析的复杂度。 前端通过以下代码发起multipartForm请求： 1234567&lt;form action=&quot;/login&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; username:&lt;input type=&quot;text&quot; name=&quot;username&quot;&gt;&lt;br&gt; password:&lt;input type=&quot;text&quot; name=&quot;password&quot;&gt;&lt;br&gt; uploadFile:&lt;input type=&quot;file&quot; name=&quot;file1&quot;&gt;&lt;br&gt; uploadFile:&lt;input type=&quot;file&quot; name=&quot;file2&quot;&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;login&quot;&gt; &lt;/form&gt; 上述代码除了传输username和password文本信息外，还额外上传两个文件。 顾名思义，multipart表单会以一块(part)一块的方式传输上面的四部分。转化为的报文如下： 123456789101112131415161718192021222324POST /login HTTP/1.1\\r\\n[[ Less interesting headers ... ]]Content-Type: multipart/form-data; boundary=---------------------------735323031399963166993862150\\r\\nContent-Length: 414\\r\\n\\r\\n-----------------------------735323031399963166993862150\\r\\n #--boundary，注意比上面的boundary多了两个-Content-Disposition: form-data; name=&quot;username&quot;\\r\\n #第一部分，username\\r\\ngu\\r\\n-----------------------------735323031399963166993862150\\r\\n #--boundaryContent-Disposition: form-data; name=&quot;password&quot;\\r\\n #第二部分，password\\r\\n123\\r\\n-----------------------------735323031399963166993862150\\r\\n #--boundaryContent-Disposition: form-data; name=&quot;file1&quot;; filename=&quot;1.txt&quot;\\r\\n #第三部分，文件1Content-Type: text/plain\\r\\n\\r\\nContent of 1.txt.\\r\\n-----------------------------735323031399963166993862150\\r\\n #--boundaryContent-Disposition: form-data; name=&quot;file2&quot;; filename=&quot;2.html&quot;\\r\\n #第四部分，文件2Content-Type: text/html\\r\\n\\r\\n&lt;!DOCTYPE html&gt;&lt;title&gt;Content of 2.html.&lt;/title&gt;\\r\\n-----------------------------735323031399963166993862150--\\r\\n #--bounadry--标识表单结束 3.multipart表单详解以上面的报文为例，15行为报文首部，624行为报文主体用于存储表单。 上述表单总共被划分为4个部分，正好对应于html代码中的四项，每一部分之间以\\r\\n–boundary作为分隔符(delimiter)，其中boundary在首部的第3行给出，服务端知道boundary之后就可以区分不同的part。表单的末尾是以\\r\\n–boundary–为结束。要注意的是分隔符是\\r\\n–boundary，相较于boundary前面多了两个-(dash)。 multipart表单转化为报文分为几个部分，有些是文本，有些是文件。文件部分的头部比文本部分的头部多一个filename。各个部分之间通过\\r\\n–boundary来区分。Boundary在最开始设置，是一串随机数字。 但事实上很多客户端的实现都不会有这些空格，同时为了让代码更紧凑，我们将不考虑出现空格的情况，因此我们的框架并没有严格按照标准，读者须知。 每一个part中又分为两部分，第一个部分为头部信息，第二个部分为消息主体，头部信息遵循MIME标准，头部信息与消息主体之间通过两个CRLF分隔。如果该part用于传输文件，则在头部信息中还会多出filename字段。 4.解析思路先看看标准库的是如何解析multipart的，我们要做的就是模仿它的API(代码忽略了所有err)： 1234567891011121314func handleUpload(w http.ResponseWriter, r *http.Request) &#123; // 生成一个MultipartReader mr, _ := r.MultipartReader() // 通过NextPart就可以获取每一个part part, _ := mr.NextPart() // 如果没有了下一个part，err会返回io.EOF // 对part读取就能不多不少获取属于该part的消息主体(没有头部信息) data, _ := ioutil.ReadAll(part) // FileName不为空，则这part承载的是文件 if part.FileName() != &quot;&quot; &#123; fmt.Printf(&quot;filename=%s, data:\\n%s\\n&quot;, part.FileName(), data) &#125; else if part.FormName() != &quot;&quot; &#123; // 否则是文本信息 fmt.Printf(&quot;formname=%s, data:\\n%s\\n&quot;, part.FormName(), data) &#125;&#125; 先通过r.MultipartReader创建了一个multipart.Reader，multipartReader的NextPart方法作为迭代器，不停调用能够得到一个个part(我们演示里只调用了一次)，这个part是multipart.Part类型，实现了Reader接口，我们可以通过调用Read方法读取属于该part的内容。如果NextPart返回io.EOF错误，则代表没有下一个part了，表单处理完毕。part通过判断filename是否为空来判断这个part是传输文件还是文本。 API的使用非常的清晰简单，以part为单位依次处理，也很符合我们的逻辑。 尽管用户使用方便，但标准库肯定把解析的复杂性封装到了内部，可以推测标准库底层做了哪些工作以及存在哪些难点： 前文讲到每个part报文是分头部和消息主体两部分的，但上述代码中对part的读取只能读取消息主体部分，说明了头部信息已经被mr.NextPart()消费、预解析后存取在了part结构体的成员中。 part的Read方法只能读取属于该part的消息主体数据，所以必须得规定读取的结束，不能让其无限制对Body随意读从而将下一个part数据读出。每个part是以boundary为分割，所以只要发现boundary就说明该part数据读取完毕了，应该让Read方法返回io.EOF。 问题就出现在将不同的part区分开并不是很容易。对于chunk编码来说，是利用chunk size来标记这一块的长度，因此很轻松就可以区分不同的块。但对于multipart来说，是利用一特殊字符串标记范围的方式，我们需要进行数据的比对，找到boundary的出现位置。 如果我们能一开始把整个http报文主体先读完然后缓存起来，那么找出所有boundary的位置会简单许多，但这显然不现实。我们只能通过开辟固定大小缓存的方式，通过滑动窗口解决问题。 part的Read方法实现的大致思路： 注意：我们会利用bufio.Reader的缓存功能完成我们的需求，能完成下面代码的前提是您能完全了解bufio.Reader，熟悉它的Read以及Peek的原理。 此处确实需要很强的抽象思维，我尽量讲述清楚，读者如果心有困惑可以结合后续代码查看。 我们对上文实现的Body分配4KB的缓存形成一个bufio.Reader，以这个缓存作为滑动窗口，每次从Body中peek出4KB的数据(peek的目的就是预查看)，然后检测这4KB数据中是否出现分隔符\\r\\n–boundary，通过分隔符就可以确立两个part之间的交界，从而就能知道当前part应该读到哪就停止。 如果没有出现分隔符，则代表peek预查看的数据就属于当前的part，我们将这4KB的数据放心读出即可。 如果出现了分隔符，分隔符之前的数据属于该part，分隔符之后的数据属于下一个part，那么这个part只允许将分隔符之前的数据读出，剩下数据等待用户调用mr.NextPart切换到下一个part后读取。 事实上，对于第一种情况就算没有出现分隔符，我们也不能将这4KB全部当作该part数据读出，因为分隔符并不是一个字符，而是一个字符串，如果这个字符串前一半出现在这4KB数据的末尾，还有一半还在IO流中待读出，我们将这前一半也当作part数据读走后，multipart的解析绝对会出现问题，因此我们一次最多只能读取4KB-len(\\r\\n–boundary)+1的数据。 5.代码实现先介绍几个结构体的意义： MultipartReader结构体 1234567891011121314151617181920212223242526272829const bufSize = 4096 // 滑动窗口大小type MultipartReader struct &#123; // bufr是对Body的封装，方便我们预查看Body上的数据，从而确定part之间边界 // 每个part共享这个bufr，但只有Body的读取指针指向哪个part的报文， // 哪个part才能在bufr上读取数据，此时其他part是无效的 bufr *bufio.Reader // 记录bufr的读取过程中是否出现io.EOF错误，如果发生了这个错误， // 说明Body数据消费完毕，表单报文也消费完，不需要再产生下一个part occurEofErr bool crlfDashBoundaryDash []byte //\\r\\n--boundary-- crlfDashBoundary []byte //\\r\\n--boundary，分隔符 dashBoundary []byte //--boundary dashBoundaryDash []byte //--boundary-- curPart *Part //当前解析到了哪个part crlf [2]byte //用于消费掉\\r\\n&#125;//传入的r将是Request的Body，boundary会在http首部解析时就得到func NewMultipartReader(r io.Reader, boundary string) *MultipartReader &#123; b := []byte(&quot;\\r\\n--&quot; + boundary + &quot;--&quot;) return &amp;MultipartReader&#123; bufr: bufio.NewReaderSize(r, bufSize), //将io.Reader封装成bufio.Reader crlfDashBoundaryDash: b, crlfDashBoundary: b[:len(b)-2], dashBoundary: b[2 : len(b)-2], dashBoundaryDash: b[2:], &#125;&#125; multipartReader结构体对象：这个结构体对象代表读取某个multipart表单对应的请求报文结构中某一个部分part，它的属性都是和读取multipart表单该部分part相关的，所绑定的方法是获取multipart表单相关的方法。比如获取下一个部分的方法nextpart（），这个方法返回一个part结构体对象，part结构体介绍在下面。 multipartReader结构体绑定了初始化方法NewmultipartReader，初始化bufr时，调用bufio包下的newReader方法，传入底层的io.reader以及缓存切片的容量即可初始化，因为bufio.reader的底层其实就是io.reader加上一个缓存切片。 因为所有Part都会在bufr上读取数据，前面part将属于它的数据消费掉之后，后续的part才能读取自己的数据。因此我们用curPart记录当前是哪个part占有了bufr，方便我们对其管理。 我们的滑动窗口大小为4KB，每次peek固定4KB大小的数据，也就意味着我们可能从bufr的底层Reader中一次最多读出4KB的数据，用来填充缓冲区。既然会对底层的Reader读取，就可能产生错误，对于非io.EOF错误，我们直接返回即可。但对于io.EOF错误，只是意味着bufr底层的Reader流中没有了数据，并不意味着bufr的缓存中没数据，因此我们需要记录是否出现了io.EOF错误，如果出现了这个错误，我们只需要将bufr缓存里的数据处理完即可。 NextPart()方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// https://www.gufeijun.com// 生成下一个part Readerfunc (mr *MultipartReader) NextPart() (p *Part, err error) &#123; if mr.curPart != nil &#123; // 将当前的Part关闭掉，即消费掉当前part数据，好让body的读取指针指向下一个part // 具体实现见后文 if err = mr.curPart.Close(); err != nil &#123; return &#125; if err = mr.discardCRLF(); err != nil &#123; return &#125; &#125; // 下一行就应该是boundary分割 line, err := mr.readLine() if err != nil &#123; return &#125; // 到multipart报文的结尾了，直接返回 if bytes.Equal(line, mr.dashBoundaryDash) &#123; return nil, io.EOF &#125; if !bytes.Equal(line, mr.dashBoundary) &#123; err = fmt.Errorf(&quot;want delimiter %s, but got %s&quot;, mr.dashBoundary, line) return &#125; // 这时Body已经指向了下一个part的报文 p = new(Part) p.mr = mr // 前文讲到要将part的首部信息预解析，好让part指向消息主体，具体实现见后文 if err = p.readHeader(); err != nil &#123; return &#125; mr.curPart = p return&#125;// 消费掉\\r\\nfunc (mr *MultipartReader) discardCRLF() (err error) &#123; if _, err = io.ReadFull(mr.bufr, mr.crlf[:]); err == nil &#123; if mr.crlf[0] != &#x27;\\r&#x27; &amp;&amp; mr.crlf[1] != &#x27;\\n&#x27; &#123; err = fmt.Errorf(&quot;expect crlf, but got %s&quot;, mr.crlf) &#125; &#125; return&#125;// 读一行func (mr *MultipartReader) readLine() ([]byte, error) &#123; return readLine(mr.bufr)&#125;// 直接利用了解析http报文首部的函数readHeader，很简单func (p *Part) readHeader() (err error) &#123; p.Header, err = readHeader(p.mr.bufr) return err&#125;// 将当前part剩余的数据消费掉，防止其报文残存在Reader上影响下一个partfunc (p *Part) Close() error &#123; if p.closed &#123; return nil &#125; _, err := io.Copy(ioutil.Discard, p) p.closed = true //标记状态为关闭 return err&#125; 实现很简单，NextPart首先会将当前的Part关闭，Close方法会将当前Part中用户未消费的数据给消费掉，防止影响下一个Part的读取。接着就是读取一行，将boundary读出，切换到下一个part，并将该part的Header解析，readHeader是往期文章的辅助函数。这时bufr的读取指针自然指向了part的消息主体部分，解决了part的读取指针初始指向的问题。 解析一下NextPart方法 这个方法也是绑定在multipart结构体上的，它的作用是返回下一个part结构体对象。如果curpart不为空，而我们此时要获取下一个part，所以要先将当前的part关闭，即消费掉当前part的数据。关闭后，读取一行数据，读到的应该就是分隔符。如果发现是表单结尾的分割符，说明表单读取结束了。如果既不是表单结尾分隔符，也不是中间分隔符，说明出错了，返回错误。此时读指针应该正好指向下一个part了，新建一个part结构体，赋值给p。p就代表下一个part，让这个p的mr属性等于当前的multipartReader结构体对象mr，代表这个part是从这个表单上读的。此时，读指针应该是指向表单中这个part的头部信息，而我们需要的是这个part的消息主体信息，所以调用readHeader方法预解析头部信息，预解析后读指针就刚好指向了这个新part的消息主体。然后将当前的multipartReader对象的curpart设定为当前这个新part，结束。nextPart方法就解析完了。 下面的重头戏就是Part结构的Read方法如何保证读取的结束了。 part结构体 12345678910type Part struct &#123; Header Header // 存取当前part的首部 mr *MultipartReader // 下两者见前面的part报文 formName string fileName string // 当该part传输文件时，fileName不为空 closed bool // part是否关闭 substituteReader io.Reader // 替补Reader parsed bool // 是否已经解析过formName以及fileName&#125; Part结构体对象：这个结构体对象实际上就是对应了multipart表单某个部分的映射。注意：上面那个对象实际上是一个读取相关的，而这里的part对象则是具体的一个部分实例。这个part结构体内部嵌套了multipartReader结构体对象这个属性。主要讲讲substituteReader，如果它不为空，我们对Part的Read则优先交给substituteReader处理，主要是为了方便引入io.LimiteReader来凝练我们的代码。substituteReader不为nil的时机，就是已经能够确定这个part还剩下多少数据可读了。这个见下面read代码就很容易理解。 part的read（）方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// https://www.gufeijun.comfunc (p *Part) Read(buf []byte) (n int, err error) &#123; // part已经关闭后，直接返回io.EOF错误 if p.closed &#123; return 0, io.EOF &#125; // 不为nil时，优先让substituteReader读取 if p.substituteReader != nil &#123; return p.substituteReader.Read(buf) &#125; bufr := p.mr.bufr var peek []byte //如果已经出现EOF错误，说明Body没数据了，这时只需要关心bufr还剩余已缓存的数据 if p.mr.occurEofErr &#123; peek, _ = bufr.Peek(bufr.Buffered()) // 将最后缓存数据取出 &#125; else &#123; //bufSize即bufr的缓存大小，强制触发Body的io，填满bufr缓存 peek, err = bufr.Peek(bufSize) //出现EOF错误，代表Body数据读完了，我们利用递归跳转到另一个if分支 if err == io.EOF &#123; p.mr.occurEofErr = true return p.Read(buf) &#125; if err != nil &#123; return 0, err &#125; &#125; //在peek出的数据中找boundary index := bytes.Index(peek, p.mr.crlfDashBoundary) //两种情况： //1.即||前的条件，index!=-1代表在peek出的数据中找到分隔符，也就代表顺利找到了该part的Read指针终点， // 给该part限制读取长度即可。 //2.即||后的条件，在前文的multipart报文，是需要boudary来标识报文结尾，然后已经出现EOF错误, // 即在没有多余报文的情况下，还没有发现结尾标识，说明客户端没有将报文发送完整，就关闭了链接， // 这时让substituteReader = io.LimitReader(-1)，逻辑上等价于eofReader即可 if index != -1 || (index == -1 &amp;&amp; p.mr.occurEofErr) &#123; p.substituteReader = io.LimitReader(bufr, int64(index)) return p.substituteReader.Read(buf) &#125; //以下则是在peek出的数据中没有找到分隔符的情况，说明peek出的数据属于当前的part //见上文讲解，不能一次把所有的bufSize都当作消息主体读出，还需要减去分隔符的最长子串的长度。 maxRead := bufSize - len(p.mr.crlfDashBoundary) + 1 if maxRead &gt; len(buf) &#123; maxRead = len(buf) &#125; return bufr.Read(buf[:maxRead])&#125; 这个part结构体绑定了一个read()方法，这个read方法实现了对这个multipart表单的这个部分的内容的正确读取。这里的read方法的作用其实就类似于上一章中从报文主体中读取chunk编码方式的内容，只不过现在是从报文主体中读取mutipart表单的内容。 Body中有数据，我们就每次取固定bufSize滑动窗口大小的数据进行处理；如果表单报文读完、没有数据了即出现io.EOF错误，我们只需要将bufr最后的缓存处理完毕即可。对应于代码的if else分支。 Peek方法出现io.EOF错误有两种情况：①一种异常：客户端主动关闭连接，提前终止。②Body的数据我们正常消费完毕了。不管是那种情况，Body都不能再读取，我们接下来只需要关注已经缓存且未处理的内容，通过bufr.Peek(bufr.Buffered())将最后缓存取出。因为第一种情况是我们不希望看到的(报文没发送完整)，第二种情况是正常情况，我们需要区分是那种原因导致了io.EOF，解决办法就是查看最后缓存中是否存在boundary(正常的multipart报文会以boundary标识结束)，存在则说明报文发送完整了，是第二种情况。 Part的Read指针应该终止在哪，也对应两种情况，一种是正常碰到该part与下一个part的分隔符，另一种是出现异常错误比如客户端主动断开连接提前终止。对于第一种情况，当前part最多能读取index个字节的数据，即将分隔符前的数据读完，对于第二种情况我们不能再读取即最多能读0B的数据，也就是说不管是哪一种情况，我们都知道这个Read方法最多还能读多少字节，从而比较巧妙地用io.LimitReader进行统一处理，代码优雅许多。对应代码的36~39行。 代码已经写了详细的注释，强烈建议读者自行思考实现一遍，才能真正理会这个解析过程。 再次梳理流程 是这样的，在文章（4）中，我们知道requset结构体中有一个属性叫作body，它是一个reader类型的属性，其实这个就是用来读取请求报文的报文主体的，但是要注意不要误认为body是用来存放报文主体的内容的，body实际上是一个接口，可以理解为他只是用来读取报文主体内容的一个方法。再回到第三章，这个chunkReader结构体实际上也是一个读取报文主体内容的接口，所以可以赋值给body。在这个chunkReader结构体内部，实际上是封装了一个属性，也即bufio包的reader，它是利用bufio包下的reader来实现读取内容的。我们知道bufio.reader里面有一个bufr切片，他其实是一个缓存切片。我们现在要干什么事呢？其实就是现在报文主体的内容不是和第三章一样那么简单了，现在报文主体内容中携带有mutipart表单了，以前读取简单的内容，现在我们读取复杂的内容，也就是利用bufio.reader从mutipart表单中读取内容，但是这个表单的内容部分分为了好几个part，不同part之间用分隔符标识。所以我们重新自定义一个切片叫peek，这个切片的大小是预先设定好了的，比如我们给它分配4kb，利用bufio.reader里面自带的缓存切片bufr的peek方法去从bufr里面预加载设定好的容量也即4kb到这个新切片peek中。此时，我们观察这个新切片peek，去看这个切片里是否含有分割符。注意：这里实际上有两个切片，一个是bufio.reader自带的切片bufr，它的作用是bufio.reader在读数据时先缓存进这个切片bufr当中。而我们自定义的新切片peek是用来存从bufr已缓存的内容中预加载的东西。bufr容量更大，peek容量设定好为4kb，每次从bufr中取出4kb放进peek。整体逻辑是bufio.reader先读，缓存进切片，然后再分情况是用io.reader的read读，还是就用bufio.reader的read方法读。如下。 如果发现切片peek中含有了分隔符，说明我们找到了part与part之间的临界点，切片中临界点后面的数据不属于当前part，此时我们规定读取的范围就到这个临界点，也就是说读取出数据，读到这个临界点时就正好把其中一个part不多不少的取出来了，但是这里并不是从切片中读。那么从哪里读取出来呢？它调用了substituReader，这个其实是一个io.reader类型的东西，io.reader是个什么东西呢？实际上io.reader就是对底层文件直接操作读取的意思，而bufr.reader就是先将底层文件预加载缓存到切片中，可以减少直接对底层文件的IO操作。所以io.reader实际上就是直接对底层文件读取，这里也就是直接对报文中body的某个part部分的读取，直接读取到临界点（通过limitreader方法限制范围）。 如果切片中没有发现分隔符，但是body数据已经读完了，此时也是直接调用io.reader来对底层数据直接读取。 如果切片中没有分隔符，也没有数据读完的报警，理论上说明此时切片中的数据就是当前part的数据，这时候我们读取数据是直接从切片中读取，而不是从底层读取。但是此时我们并不能安心的把切片中所有数据作为当前part的数据，因为有可能出现一种意外情况，就是分隔符还挺长的，如果一半出现则切片中，另一半还在底层文件，此时切片中也没有发现分隔符，但实际上切片中存在一半的分隔符，这部分内容不属于当前part的数据，需要剔除。这种情况无法针对性解决。只能从整体上防止这种情况的发生。通过设置从切片中读取的范围来防止这种情况，设置的读取长度length&#x3D;切片的容量减去分隔符的长度，这样可以始终保证切片从0到length范围内在这种情况下一定是当前part的数据，从length到切片结尾这段无法保证，就不读它，留着下一段缓存进来的时候再判断，这是因为切片有一个机制，你如果切片里内容没读完的话，本轮结束时它会清空内容，将剩余内容返回到底层body中去，这样即使没读完，下一轮预加载同样也可以接着上一轮的结尾了（这个设定在nextpart方法中，当前part结束时，会关闭当前part，并消费剩余内容）。 注意：这种情况下，我们读数据直接调用切片的read方法，即bufio.reader类型对象的read方法。Part的read方法到此结束。 再详解一下part的read方法具体代码 若发现当前part已被关闭，则不能再读，直接返回读取0字节和错误。如果发现substitueReader属性不为空，说明这个属性被赋过值了，这个属性的类型是io.reader类型的，上面我们提到如果发现切片含有分割符，我们要用io.reader直接从底层读取，因为这样可以用limitreader方法来保证恰好读取到分隔符。所以如果发现substitureader不为空，说明找到了分割符，我们就直接用substitureader的read方法读取。将当前part的所属表单的读接口bufr赋值给bufr，自定义了一个缓存切片叫peek。如果发现occurEofErr为true，说明读不到数据了，也即这个part的内容读完了，而我们一开始都是用bufio.reader将数据全部装入缓存切片中的，所以此时我们只需要将缓存切片中的剩余内容全部提取出来装进peek切片即可。如果没有错误，说明part的内容还没读完，我们只需要从缓存切片中提取4kb内容放进peek切片即可。也有可能缓存切片此时没有4kb的内容，报错了，但是此时occurEofErr还是false，所以我们将它设定为true，重新递归调用part的read方法，这样就会走上面那个将缓存切片剩余内容提取。此时peek中已有数据，我们在里面找分隔符的索引。如果发现索引不等于-1，说明在peek里面找到了分隔符，此时要用io的Limitreader来读到索引为止。或者没有找到分隔符，但是却出现了eof错误，说明客户端没有将报文发送完整，就关闭了连接，此时也用Limitreader来读到-1。若没有找到分隔符，如上，不能一次读完。 关于Part最硬核的操作已经完成，下面就是简单的工作。 为part提供获取formName以及fileName的方法，和http解析首部极为类似，就是简单的字符串处理，对比上述的part首部字段报文自行观看： 1234567891011121314151617181920212223242526272829303132333435363738394041// 获取FormNamefunc (p *Part) FormName() string &#123; if !p.parsed &#123; p.parseFormData() &#125; return p.formName&#125;// 获取FileNamefunc (p *Part) FileName() string &#123; if !p.parsed &#123; p.parseFormData() &#125; return p.fileName&#125;func (p *Part) parseFormData() &#123; p.parsed = true cd := p.Header.Get(&quot;Content-Disposition&quot;) ss := strings.Split(cd, &quot;;&quot;) if len(ss) == 1 || strings.ToLower(ss[0]) != &quot;form-data&quot; &#123; return &#125; for _, s := range ss &#123; key, value := getKV(s) switch key &#123; case &quot;name&quot;: p.formName = value case &quot;filename&quot;: p.fileName = value &#125; &#125;&#125;func getKV(s string) (key string, value string) &#123; ss := strings.Split(s, &quot;=&quot;) if len(ss) != 2 &#123; return &#125; return strings.TrimSpace(ss[0]), strings.Trim(ss[1], `&quot;`)&#125; 比较简单，不再赘述。 request.go MultipartReader已经搞定，万事具备，只欠东风。下面要做的就是从http协议的首部字段中解析出boundary，然后交给MultipartReader即可。 在request.go中增加如下方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344type Request struct &#123; // 本章给Request新增的属性 contentType string boundary string // ....&#125;func readRequest(c *conn) (r *Request, err error) &#123; // .... // 记得在readRequest中调用parseContentType r.parseContentType() return r, nil&#125;// boundary是存取在Content-Type字段中func (r *Request) parseContentType() &#123; ct := r.Header.Get(&quot;Content-Type&quot;) //Content-Type: multipart/form-data; boundary=------974767299852498929531610575 //Content-Type: multipart/form-data; boundary=&quot;&quot;------974767299852498929531610575&quot; //Content-Type: application/x-www-form-urlencoded index := strings.IndexByte(ct, &#x27;;&#x27;) if index == -1 &#123; r.contentType = ct return &#125; if index == len(ct)-1 &#123; return &#125; ss := strings.Split(ct[index+1:], &quot;=&quot;) if len(ss) &lt; 2 || strings.TrimSpace(ss[0]) != &quot;boundary&quot; &#123; return &#125; // 将解析到的CT和boundary保存在Request中 r.contentType, r.boundary = ct[:index], strings.Trim(ss[1],`&quot;`) return&#125;// 得到一个MultipartReaderfunc (r *Request) MultipartReader()(*MultipartReader,error)&#123; if r.boundary==&quot;&quot;&#123; return nil,errors.New(&quot;no boundary detected&quot;) &#125; return NewMultipartReader(r.Body,r.boundary),nil&#125; 前面我们定义了multipartReader结构体，他其实对应了读取一个表单的结构体。里面有一个分隔符boundary属性，这个属性需要从http请求的表单中解析出来才能拿到。解析表单分隔符的工作在readRequest方法中调用。 ParsecontentType方法解析出分隔符。 解析出来以后赋值给request结构体的分隔符属性。另外，这个multipartreader结构体里面还有一个bufio.reader属性，这个应该就是和chunkReader结构体里面一样的，因为这两个结构体的意义其实是一样的嘛，都是用来读取报文主体内容的。 6.测试测试很简单，对于前段传输的multipart表单，如果part传输的是文本，我们将其输出到中断，如果是文件，我们对文件保存到硬盘上。 main.go代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package mainimport ( &quot;example/httpd&quot; &quot;fmt&quot; &quot;io&quot; &quot;log&quot; &quot;os&quot;)type myHandler struct&#123;&#125;func (*myHandler) ServeHTTP(w httpd.ResponseWriter, r *httpd.Request) &#123; mr, err := r.MultipartReader() if err != nil &#123; log.Println(err) return &#125; var part *httpd.Partlabel: for &#123; part, err = mr.NextPart() if err != nil &#123; break &#125; // 判断是文本part还是文件part switch part.FileName() &#123; case &quot;&quot;: //文本 fmt.Printf(&quot;FormName=%s, FormData:\\n&quot;, part.FormName()) // 输出到终端 if _, err = io.Copy(os.Stdout, part); err != nil &#123; break label &#125; fmt.Println() default: //文件 // 打印文件信息 fmt.Printf(&quot;FormName=%s, FileName=%s\\n&quot;, part.FormName(), part.FileName()) var file *os.File if file, err = os.Create(part.FileName()); err != nil &#123; break label &#125; if _, err = io.Copy(file, part); err != nil &#123; file.Close() break label &#125; file.Close() &#125; &#125; if err != io.EOF &#123; fmt.Println(err) &#125; // 发送响应报文 io.WriteString(w, &quot;HTTP/1.1 200 OK\\r\\n&quot;) io.WriteString(w, fmt.Sprintf(&quot;Content-Length: %d\\r\\n&quot;, 0)) io.WriteString(w, &quot;\\r\\n&quot;)&#125;func main() &#123; svr := &amp;httpd.Server&#123; Addr: &quot;127.0.0.1:80&quot;, Handler: new(myHandler), &#125; panic(svr.ListenAndServe())&#125; 利用curl测试我们的接口： 12#发送两个文本，上传两个文件：1.md以及2.md$ curl -F &quot;username=gu&quot; -F &quot;password=123&quot; -F &quot;file1=@1.md&quot; -F &quot;file2=@2.md&quot; http://127.0.0.1 服务端输出： 123456FormName=username, FormData:guFormName=password, FormData:123FormName=file1, FileName=1.mdFormName=file2, FileName=2.md 查看服务端程序所在目录，正确保存了1.md以及2.md文件，且保存的文件hash值与原文件相同。 7.总结本文完成了multipart&#x2F;form-data表单的解析，难度比较高，需要多加理解。但我们提供的解析表单API依旧比较繁琐，下一文会在今天的基础上，再做封装。","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-（5）","slug":"从零实现Golang的HTTP标准库-（5）","date":"2022-10-08T00:20:00.000Z","updated":"2022-10-06T09:23:19.192Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（5）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%885%EF%BC%89.html","excerpt":"","text":"1.概述前文进行了Request首部字段的解析，已经完整的支持了GET请求。对于POST请求来说，其传送的表单数据在HTTP请求的报文主体body部分，服务端需要进一步IO操作才能读出，为了提高性能我们将POST表单的解析权交给用户，为此我们给Request结构体封装一个Body字段，作为IO的接口。 2.如何知道报文主体的长度以下一段http协议post请求报文为例： 12345678910POST /index?name=gu HTTP/1.1\\r\\n #请求行Content-Type: text/plain\\r\\n #此处至报文主体为首部字段User-Agent: PostmanRuntime/7.28.0\\r\\nHost: 127.0.0.1:8080\\r\\nAccept-Encoding: gzip, deflate, br\\r\\nConnection: keep-alive\\r\\nCookie: uuid=12314753; tid=1BDB9E9; HOME=1\\r\\nContent-Length: 18\\r\\n\\r\\nhello,I am client! #报文主体 报文主体与首部字段之间通过两个\\r\\n(CRLF)为分隔。前文中我们框架已经解析出了首部字段，本文的任务就是解析报文主体部分。 报文主体就是用于携带客户端的额外信息，Body报文主体部分只有post或者put请求才会有。由于报文主体中能包含任何信息，更是不限长度，所以http协议就不能像首部字段一样以某个字符如CRLF为边界，来标记报文主体的范围。那么客户端是怎么保证服务端能够完整的不多不少的读出报文主体数据呢？ 其实很简单，我们只要在首部字段中用一项标记报文主体长度，就解决了问题。就以上述报文为例，首部字段中包含一个Content-Length字段，其值为18，服务端随后从tcp连接中读取18个字节的数据，就正好把hello,I am client!这18B的数据读出，恰恰满足我们的需要。 当然除了使用Content-Length之外，http还可以使用chunk编码的方式，这里先埋个伏笔。 3.需求分析还是得明确一点，http报文的头部部分很短，上一章中框架将这部分读取并解析后直接交给用户，既省时也省力。 但问题是http的报文主体是不限长度的，框架无脑将这些字节数据读出来，是很糟糕的设计。 最明智的做法是，将这个解析的主动权交给用户，框架只提供方便用户读取解析报文主体的接口而已。 因此在上文中，Request中存在一个Body(io.Reader接口类型)字段，用户对Body的读取就正好能读出对应的报文主体。 仅仅让Body能读取到报文主体还不行，为了防止读多或读少，用户还需要限定只读取上述的Content-Length的长度，让用户手动指定读取长度的方式很麻烦也极为出错，我们想要的是能够达到以下效果： 12345678func (*myHandler) ServeHTTP(w httpd.ResponseWriter,r *httpd.Request)&#123; // 不需要指定长度就能将报文主体不多不少读出 buf,err:=ioutil.ReadAll(r.Body) if err!=nil&#123; return &#125; fmt.Printf(&quot;%s\\n&quot;,buf)&#125; 要保证Body达到我们期望的行为，这就意味着Body提供的Read方法能够保证以下两点： 对Body读取的这个指针一开始应该指向报文主体的开头，也就是说不能将报文主体前面的首部字段读出。规定了读取的起始。 多个http的请求相继在tcp连接上传输，当前http请求的Body就应该只能读取到当前请求的报文主体，即只能读取Content-Length长度的数据。规定了读取的结束。 如果单纯保证第一点，完全可以用上一文中conn结构体的bufr字段作为Body，因为我们已经将首部字段从bufr中读出，下一次对bufr的读取自然会从报文主体开始。 但这样做，第二点就无法满足。在go语言中，对一个io.Reader的读取，如果返回io.EOF错误代表我们将这个Reader中的所有数据读取完了。ioutil.ReadAll就是利用了这个特点，如果不出现一些异常错误，它会不停的读取数据直至出现io.EOF。而一个网络连接net.Conn，只有在对端主动将连接关闭后，对net.Conn的Read才会返回io.EOF错误。所以就意味着如果服务端出现以下代码： 12conn,_ := listener.Accept()ioutil.ReadAll(conn) 只要客户端不主动关闭连接，我们也不设置超时事件，我们会永久的阻塞在第二行处。 将bufr设置成Body就有这个问题，对bufr的读取最终会落到底层tcp连接(net.Conn)的读取，就算我们把当前请求的报文主体读取出来了，只要客户端不关闭连接，我们会永久阻塞。要是客户端继续发送了下一个http请求，我们当前的body还会顺带把下一个请求报文读出来，这绝对是不满足我们的需求的。 所以必须保证Body这个Reader在将报文主体读取完毕即读取Content-Length长的数据后，立即返回一个io.EOF错误，通知ReadAll函数我们已经读取完毕了！ 现在的任务就是好好思量，Body的Read方法该如何设置，从而达到想要的效果。 4.代码实现思维敏锐的朋友可能已经有了解决方案，那就是用io.LimitReader！ 上文用它来解决无限解析首部字段的问题，这里又派上用场。我们只需要将N设置成Content-Length，在最多读取N字节的长度后，LimitReader就会返回io.EOF错误，问题也迎刃而解。 在readRequest中有一个setupBody方法，其实readRequest方法就是从一个请求报文中读取数据从而创建一个request对象。所以setupBody方法其实就是从报文中解析报文主体部分来填充request对象的body属性。同时在解析出报文主体赋给body属性的同时，还规定了body的一些规则：比如规定body读取的起始以及结尾。 更改request.go的setupBody方法： 12345678910111213141516func (r *Request) setupBody() &#123; if r.Method != &quot;POST&quot; &amp;&amp; r.Method != &quot;PUT&quot; &#123; r.Body = &amp;eofReader&#123;&#125; //POST和PUT以外的方法不允许设置报文主体 &#125;else if cl := r.Header.Get(&quot;Content-Length&quot;); cl != &quot;&quot; &#123; //如果设置了Content-Length contentLength, err := strconv.ParseInt(cl, 10, 64) if err != nil &#123; r.Body = &amp;eofReader&#123;&#125; return &#125; // 允许Body最多读取contentLength的数据 r.Body = io.LimitReader(r.conn.bufr, contentLength) &#125;else&#123; r.Body = &amp;eofReader&#123;&#125; &#125;&#125; 对于非PUT以及POST方法，按照http协议的规定是不允许设置报文主体的，所以我们将Body设置成eofReader就好，对它的Read只会返回io.EOF错误。 5.chunk上面这种方法是针对客户端发请求过来，带有content-length的情况，还有一种情况不带content-length，而是采用chunk编码的方式分块传输。 除了通过Content-Length通知对端报文主体的长度外，http1.1引入了新的编码方式——chunk编码(分块传输编码)，顾名思义就是会将报文主体分块后进行传输。 利用Content-Length存在一个问题：我们需要事先知道待发送的报文主体长度，而有些时候我们是希望数据边产生边发送，根本无从知道将要发送多少的数据。因此http1.1相较http1.0除了长连接之外的另一大改进就是引入了chunk编码，客户端需要在请求头部设置Transfer-Encoding: chunked。 chunk编码的示例： 12345678910111213HTTP/1.1 200 OK\\r\\nContent-Type: text/plain\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n# 以下为body17\\r\\n #chunk sizehello, this is chunked \\r\\n #chunk dataD\\r\\n #chunk sizedata sent by \\r\\n #chunk data7\\r\\n #chunk sizeclient!\\r\\n #chunk data0\\r\\n\\r\\n #end 每一分块中包含两部分： 第一部分为chunk size，代表该块chunk data的长度，利用16进制表示。 第二部分为chunk data，该区域存储有效载荷，实际欲传输的数据存储在这部分。 chunk size与chunk data之间都利用\\r\\n作为分割，通过0\\r\\n\\r\\n来标记报文主体的结束。 6.chunkReader这个时候body属性的赋值就不是像content-length那样了，而是给body赋值了一个chunkReader结构体对象。 这个chunkreader结构体对象同样要规定body可读的开头和结尾。这个chunkreader结构体对象有一个read方法，调用read（）方法可以读取出chunk编码内的报文内容。 在readRequest方法中的setupbody方法中，我们需要增加一个判断，判断当前请求是否采用了chunk编码方式，如果采用了chunk编码方式，就将chunkreader结构体对象赋值给body属性，否则还按content -length处理。 显然，我们对Body的设置就需要分类讨论了！ 抽象出一个chunkReader结构体，当客户端利用chunk传输报文主体时，我们将Body设置成chunkReader即可。那么这个chunkReader需要满足什么功能呢？ 依旧是满足上述Body的两点：规定Body读取的起始以及结尾。起始已经满足，重点考虑结尾的设计，我们这就不能使用LimitReader了，既然chunk编码的结束标志是0\\r\\n\\r\\n，那么我们的Read方法在碰到0\\r\\n\\r\\n时返回io.EOF错误即可，不允许继续向下读，因为后续的字节数据是属于下一个http请求。 如果仅做到将报文主体不多不少读出，但读取的数据包含chunk编码的控制信息(chunk size以及CRLF)，而我们只关心chunk data部分，还需要用户手动解码，这也是不可取的。 所以我们的chunkReader还需要具有解码chunk的功能，保证用户调用到的Read方法只读到有效载荷(chunk data)：hello, this is chunked data sent by client!。 新建chunk.go文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576type chunkReader struct &#123; //当前正在处理的块中还剩多少字节未读 n int bufr *bufio.Reader //利用done来记录报文主体是否读取完毕 done bool crlf [2]byte //用来读取\\r\\n&#125;func (cw *chunkReader) Read(p []byte) (n int, err error) &#123; // 报文主体读取完后，不允许再读 if cw.done &#123; return 0, io.EOF &#125; // 当前块没数据了，准备读取下一块 if cw.n == 0 &#123; // 先获取chunk Size cw.n, err = cw.getChunkSize() if err != nil &#123; return &#125; &#125; // 获取到的chunkSize为0，说明读到了chunk报文结尾 if cw.n == 0 &#123; cw.done = true //将最后的CRLF消费掉，防止影响下一个http报文的解析 err = cw.discardCRLF() return &#125; //如果当前块剩余的数据大于欲读取的长度 if len(p) &lt;= cw.n &#123; n, err = cw.bufr.Read(p) cw.n -= n return n, err &#125; //如果当前块剩余的数据不够欲读取的长度，将剩余的数据全部取出返回 n, _ = io.ReadFull(cw.bufr, p[:cw.n]) cw.n = 0 //记得把每个chunkData后的\\r\\n消费掉 if err = cw.discardCRLF(); err != nil &#123; return &#125; return&#125;func (cw *chunkReader) discardCRLF() (err error) &#123; if _, err = io.ReadFull(cw.bufr, cw.crlf[:]); err == nil &#123; if cw.crlf[0] != &#x27;\\r&#x27; || cw.crlf[1] != &#x27;\\n&#x27; &#123; return errors.New(&quot;unsupported encoding format of chunk&quot;) &#125; &#125; return&#125;func (cw *chunkReader) getChunkSize() (chunkSize int, err error) &#123; line, err := readLine(cw.bufr) if err != nil &#123; return &#125; //将16进制换算成10进制 for i := 0; i &lt; len(line); i++ &#123; switch &#123; case &#x27;a&#x27; &lt;= line[i] &amp;&amp; line[i] &lt;= &#x27;f&#x27;: chunkSize = chunkSize*16 + int(line[i]-&#x27;a&#x27;) + 10 case &#x27;A&#x27; &lt;= line[i] &amp;&amp; line[i] &lt;= &#x27;F&#x27;: chunkSize = chunkSize*16 + int(line[i]-&#x27;A&#x27;) + 10 case &#x27;0&#x27; &lt;= line[i] &amp;&amp; line[i] &lt;= &#x27;9&#x27;: chunkSize = chunkSize*16 + int(line[i]-&#x27;0&#x27;) default: return 0, errors.New(&quot;illegal hex number&quot;) &#125; &#125; return&#125; 我们用一个结构体成员n来记录当前块剩余未处理的的chunk data长度，一旦该块的数据读取完毕后就通过chunk size获取下一块的长度。 chunk编码的控制信息全部在Read方法内部消费掉了，外界能读取的仅仅只有chunk data。 详解chunk.go 首先我们定义一个chunkReader的结构体，这个结构体如果被赋值给请求对象的body属性，就代表我们将采用chunk编码的方式来读取报文主体。body属性的类型是io.reader，所以这个chunkReader类型也是reader类型的，这个结构体里面有几个属性，n代表当前正在读取的chunk块还剩多少字节未读。bufr是一个bufio.reader类型的对象，等于是这个reader里面封装了一个bufio.reader，主要是用这个bufr来读。一个bool类型的done表示所有的块是否被读完，也即报文主体是否被读完。最后有一个切片名叫crlf，用来读取换行符。 详解chunkReader的read（）方法 cw是chunkReader结构体的对象，若发现cw的done属性为true，说明报文主体全部读取完毕，返回io.eof错误停止读。如果发现cw的n属性为0，说明当前读取的chunk块读完了，应当读取下一个chunk块。所以调用getchunksize方法获取下一个chunk块的长度赋值给n，代表下一个chunk块还有n长度未读。如果此时发现n还是为0，说明读完了，将done设置为true，将最后的crlf消费掉，防止影响下一个http报文的解析。如果发现切片p的长度&lt;当前chunk中未读取的长度，说明一次读不完，要分几次来读。所以我们先调用那个bufio类型的reader里面的那个read方法，这个read方法可以看下面的蓝色链接，这个read方法返回的是已经读取的字节数，所以我们调用这个read先读取一部分数据进入p中，并获取了已经读取的字节数，再让当前cw属性的n（代表未读数）减去已经读取的字节数，就是还剩下的未读数。调用bufio.reader的目的就是为了让块剩余未读数量小于切片p的长度。如果发现切片p的长度&gt;当前chunk中未读取的长度，说明可以一次读完，直接将块中剩下的数据全部读到切片p中，这里用的方法是io.readfull，这个方法应该就会在将数据放入p的同时将p中读到的数据上传上去。将cw的n属性设为0。消费crlf。read方法至此详解结束。 别忘了更改request.go中的setupBody方法： 12345678910111213141516171819202122232425// 对端是否使用了chunk编码func (r *Request) chunked() bool &#123; te := r.Header.Get(&quot;Transfer-Encoding&quot;) return te == &quot;chunked&quot;&#125;func (r *Request) setupBody() &#123; if r.Method != &quot;POST&quot; &amp;&amp; r.Method != &quot;PUT&quot; &#123; r.Body = &amp;eofReader&#123;&#125; //POST和PUT以外的方法不允许设置报文主体 &#125;else if r.chunked()&#123; r.Body = &amp;chunkReader&#123;bufr: r.conn.bufr&#125; r.fixExpectContinueReader() //见下文 &#125; else if cl := r.Header.Get(&quot;Content-Length&quot;); cl != &quot;&quot; &#123; //如果设置了Content-Length contentLength, err := strconv.ParseInt(cl, 10, 64) if err != nil &#123; r.Body = &amp;eofReader&#123;&#125; return &#125; r.Body = io.LimitReader(r.conn.bufr, contentLength) r.fixExpectContinueReader() &#125;else&#123; r.Body = &amp;eofReader&#123;&#125; &#125;&#125; 做到这一点后，不论客户端时使用Content-Length还是chunk编码，用户看到的Body这个Reader都具有一样的行为即不多不少读到有效载荷，用户不需要自己手动分类处理，这就是封装的思想以及go语言接口的妙用。 注意：这里其实只是对body属性赋值了chunkreader对象，而没有去调用chunkreader的read方法，那是在哪里去调用的这个read方法呢？其实是在main文件测试中，调用listenandserve方法，然后进去调用serve方法，再调用serveHTTP方法，在serveHTTP方法中，我们调用ioutil.readAll方法来读取请求的body属性，在这个底层才调用了read方法，将读到的内容存入buf中，然后再利用serveHTTP方法的第一个参数：这个ResponseWriter类型的w，来往w里面写入buf，相当于将读到的报文主体内容原封不动写回去来构造一个回显响应。这个serveHTTP方法的第一个参数我原来一直以为就是响应，其实不是，它是一个用来写响应的输入器。 细心的你应该发现了我们多出了一个fixExpectContinueReader方法。这是因为为了防止资源的浪费，有些客户端在发送完http首部之后，发送body数据前，会先通过发送Expect: 100-continue查询服务端是否希望接受body数据，服务端只有回复了HTTP&#x2F;1.1 100 Continue客户端才会再次发送body。因此我们也要处理这种情况： 1234567891011121314151617181920212223242526type expectContinueReader struct&#123; // 是否已经发送过100 continue wroteContinue bool r io.Reader w *bufio.Writer&#125;func (er *expectContinueReader) Read(p []byte)(n int,err error)&#123; //第一次读取前发送100 continue if !er.wroteContinue&#123; er.w.WriteString(&quot;HTTP/1.1 100 Continue\\r\\n\\r\\n&quot;) er.w.Flush() er.wroteContinue = true &#125; return er.r.Read(p)&#125;func (r *Request) fixExpectContinueReader() &#123; if r.Header.Get(&quot;Expect&quot;) != &quot;100-continue&quot; &#123; return &#125; r.Body = &amp;expectContinueReader&#123; r: r.Body, w:r.conn.bufw, &#125;&#125; 实现也很简单，一旦发现客户端的请求报文的首部中存在Expect: 100-continue，那么我们在第一次读取body时，也就意味希望接受报文主体，expectContinueReader会自动发送HTTP&#x2F;1.1 100 Continue。 我们的框架目前依旧存在一个问题，如果用户在Handler的回调函数中没有去读取Body的数据，就意味着处理同一个socket连接上的下一个http报文时，Body未消费的数据会干扰下一个http报文的解析。所以我们的框架还需要在Handler结束后，将当前http请求的数据给消费掉。给Request增加一个finishRequest方法，以后的一些善尾工作都将交给它： 123456789func (r *Request) finishRequest() (err error)&#123; //将缓存中的剩余的数据发送到rwc中 if err=r.conn.bufw.Flush();err!=nil&#123; return &#125; //消费掉剩余的数据 _,err = io.Copy(ioutil.Discard,r.Body) return err&#125; 在conn.go的serve方法中调用finishRequest： 12345678910111213141516171819202122func (c *conn) serve()&#123; defer func() &#123; if err:=recover();err!=nil&#123; log.Printf(&quot;panic recoverred,err:%v\\n&quot;,err) &#125; c.close() &#125;() //http1.1支持keep-alive长连接，所以一个连接中可能读出 //多个请求，因此实用for循环读取 for&#123; req,err:=c.readRequest() if err!=nil&#123; handleErr(err,c) return &#125; res:=c.setupResponse() c.svr.Handler.ServeHTTP(res,req) if err=req.finishRequest();err!=nil&#123; return &#125; &#125;&#125; 7.测试目前已经可以很方便的解析报文主体了，我们的测试代码如下： 1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;example/httpd&quot; &quot;fmt&quot; &quot;io&quot; &quot;io/ioutil&quot;)type myHandler struct&#123;&#125;// echo回显服务器，将客户端的报文主体原封不动返回func (*myHandler) ServeHTTP(w httpd.ResponseWriter, r *httpd.Request) &#123; buf, err := ioutil.ReadAll(r.Body) if err != nil &#123; return &#125; const prefix = &quot;your message:&quot; io.WriteString(w, &quot;HTTP/1.1 200 OK\\r\\n&quot;) io.WriteString(w, fmt.Sprintf(&quot;Content-Length: %d\\r\\n&quot;, len(buf)+len(prefix))) io.WriteString(w, &quot;\\r\\n&quot;) io.WriteString(w, prefix) w.Write(buf)&#125;func main() &#123; svr := &amp;httpd.Server&#123; Addr: &quot;127.0.0.1:80&quot;, Handler: new(myHandler), &#125; panic(svr.ListenAndServe())&#125; 利用curl测试，使用Content-Type方式： 12345678# -d用于加上报文主体curl -d &quot;hello, this is chunked message from client!&quot; http://127.0.0.1 -i输出：HTTP/1.1 200 OKContent-Length: 56your message:hello, this is chunked message from client! 使用chunk编码的方式： 1234567$ curl -H &quot;Transfer-Encoding: chunked&quot; -d &quot;hello, this is chunked message from client!&quot; http://127.0.0.1 -i输出：HTTP/1.1 200 OKContent-Length: 56your message:hello, this is chunked message from client! 不论是用哪一种方法传输报文主体，服务端都能够正确的解析。 客户端的Form表单以报文主体为载体，所以Body的设置是解析Form表单的基础，下一文将对两种表单：application&#x2F;x-www-form-urlencoded以及multipart&#x2F;form-data进行解析。 8.总结这章整体的一个逻辑是：request结构体对象c会绑定一个readRequest方法，这个readRequest方法是用来构造serveHTTP方法的第二个参数的req的，而这个readRequest方法内部会解析请求行，解析首部字段，这两个都是上一章的内容。同时readRequest方法内部还会调用setupBody方法，这个就是解析报文主体，就是我们这一章的内容。setupBody里面会判断是采用chunk方法还是采用content-length方法，然后分别构造body。其中若采用chunk方法，就会给body属性赋值chunkReader对象。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-（4）","slug":"从零实现Golang的HTTP标准库-（4）","date":"2022-10-07T00:20:00.000Z","updated":"2022-10-06T06:47:54.562Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（4）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%884%EF%BC%89.html","excerpt":"","text":"1.概述本文将正式开始HTTP协议的解析，主要完成Request结构体的属性组成，HTTP请求报文首部字段以及请求行的解析。 2.项目结构因为在这一章中，我们会完成客户端请求报文中首部字段的解析，首部字段为一个个键值对，显而易见的是使用map结构进行一个存储，所以增加header.go文件，其中存在Header结构体，专门用于存放请求报文中请求行的键值对。此处并非重点也极为简单，所以直接从标准库中拷贝，读者自行理解： 1234567891011121314151617181920212223242526package httpdtype Header map[string][]stringfunc (h Header) Add(key, value string) &#123; h[key] = append(h[key], value)&#125;// 插入键值对func (h Header) Set(key, value string) &#123; h[key] = []string&#123;value&#125;&#125;// 获取值，key不存在则返回空func (h Header) Get(key string) string &#123; if value, ok := h[key]; ok &amp;&amp; len(value) &gt; 0 &#123; return value[0] &#125; else &#123; return &quot;&quot; &#125;&#125;// 删除键func (h Header) Del(key string) &#123; delete(h, key)&#125; Header是一个map类型，用来存储请求报文中首部字段的所有键值对。并且绑定了增删改查键值对的方法。 3.需求分析我们需要为Request结构体增加相应的属性，就应该从http请求报文出发，看看我们需要保存哪些信息。一段http请求报文： 12345678910POST /index?name=gu HTTP/1.1\\r\\n #请求行Content-Type: text/plain\\r\\n #此处至报文主体为首部字段User-Agent: PostmanRuntime/7.28.0\\r\\nHost: 127.0.0.1:8080\\r\\nAccept-Encoding: gzip, deflate, br\\r\\nConnection: keep-alive\\r\\nCookie: uuid=12314753; tid=1BDB9E9; HOME=1\\r\\nContent-Length: 18\\r\\n\\r\\nhello,I am client! #报文主体 http的请求报文分为三部分： 请求行(第一行)分别由方法Method、请求路径URL以及协议版本Proto组成。将这三者加入到Request结构即可。 首部字段由一个个键值对组成，我们的头部信息就存放在此处。这部分就用上面讲到了的Header存储。 报文主体部分，相较于前面两个更为复杂，可能具有不同的编码方式，长度也可能特别大。平时前端提交的form表单就放置在报文主体部分。仅只有POST和PUT请求允许携带报文主体。 本篇只涉及前两部分的解析，读者需知。 除此之外，像cookie以及queryString(如上面的URL中的?name&#x3D;gufeijun)，是日常开发经常使用到的部分，为了方便用户的获取，我们分别用cookies以及queryString这两个map去保存解析后的字段。因此Request结构体如下： 123456789101112type Request struct&#123; Method string //请求方法，如GET、POST、PUT URL *url.URL //URL Proto string //协议以及版本 Header Header //首部字段 Body io.Reader //用于读取报文主体 RemoteAddr string // 客户端地址 RequestURI string //字符串形式的url conn *conn //产生此request的http连接 cookies map[string]string //存储cookie queryString map[string]string //存储queryString&#125; URL隶属于uri。 URI 就是由某个协议方案表示的资源的定位标识符。采用http协议时，协议方案就是http。URI 用字符串标识某一互联网资源，而 URL表示资源的具体地点（互联网上所处的位置）。可见 URL是 URI 的子集。 Body字段在下一章中讲解，读者只需要知道用户可以对(*Request).Body读取，进而读出当前请求的报文主体即可。 4.readRequest上一章我们将框架的骨干搭好后，这里开始第一个空白处readRequest的填充，它的作用就是从tcp字节流中解析http报文，从而封装出一个Request对象。 直接上代码，请对比着上述的http报文观看： 1234567891011121314151617181920212223242526272829303132func readRequest(c *conn)(r *Request,err error)&#123; r = new(Request) r.conn = c r.RemoteAddr = c.rwc.RemoteAddr().String() //读出第一行,如：Get /index?name=gu HTTP/1.1 line, err := readLine(c.bufr) if err != nil &#123; return &#125; // 按空格分割就得到了三个属性 _, err = fmt.Sscanf(string(line), &quot;%s%s%s&quot;, &amp;r.Method, &amp;r.RequestURI, &amp;r.Proto) if err != nil &#123; return &#125; // 将字符串形式的URI变成url.URL形式 r.URL, err = url.ParseRequestURI(r.RequestURI) if err != nil &#123; return &#125; //解析queryString r.parseQuery() //读header r.Header, err = readHeader(c.bufr) if err != nil &#123; return &#125; const noLimit = (1&lt;&lt;63)-1 r.conn.lr.N = noLimit //Body的读取无需进行读取字节数限制 //设置body r.setupBody() return r, nil&#125; 对readRequest方法的解读： 新建一个request的实例r，这个request的conn属性就是c；客户端地址就调用tcp连接的方法得到；readline用来读取出一行数据，传入的参数为bufr；这个读出来的是第一行，所以就是请求行的三个属性。 用到了一个parseRequsetURI方法，将URI转化为了URL，前面不是提到URL是URI的子集吗，这个应该就是一个转化，因为我们在第一行读取到的是URI，现在通过这个方法就可以拿到URL。又调用parseQuery（）方法解析&amp;&#x3D;？那个，解析出来一个个键值对，存放到一个叫querystring的map中。 再调用readHeader（）方法，这个方法同样是传入bufr，会返回一个Header。这个Header前面介绍过，就是一个map。用来存储首部字段里的所有键值对的。 最后调用setbody（）方法设置报文主体。 6~16行为解析请求行，分别读出Method、URI以及Proto。21行解析queryString。23行读取首部字段。30行设置Body。上述代码中有四个辅助函数：readLine、parseQuery、readHeader以及setupBody，这四个我们一个一个单独讲解。 5.readLine()上文提到bufio.Reader具有ReadLine方法，其存在三个返回参数line []byte, isPrefix bool, err error，line和err都很好理解，但为什么还多出了一个isPrefix参数呢？这是因为ReadLine会借助到bufio.Reader的缓存切片(见上篇)，如果一行大小超过了缓存的大小，这也会无法达到读出一行的要求，这时isPrefix会设置成true，代表只读取了一部分。 因此我们需要对ReadLine方法进行封装，得到readLine函数，能够保证缓存容量不足的情况下也能读出一行，如下： 123456789101112131415func readLine(bufr *bufio.Reader) ([]byte, error) &#123; p, isPrefix, err := bufr.ReadLine() if err != nil &#123; return p, err &#125; var l []byte for isPrefix &#123; l, isPrefix, err = bufr.ReadLine() if err != nil &#123; break &#125; p = append(p, l...) &#125; return p, err&#125; 只要isPrefix一直为true，我们则一直读取，并将读取的部分汇总在一起，直至读到\\r\\n。 6.parseQuery()对于客户端访问的url，如127.0.0.0?name&#x3D;gu&amp;token&#x3D;1234，其中name&#x3D;gu&amp;token&#x3D;1234即为queryString字段，将这部分以KV方式存入map中。 1234567891011121314151617func (r *Request) parseQuery() &#123; //r.URL.RawQuery=&quot;name=gu&amp;token=1234&quot; r.queryString = parseQuery(r.URL.RawQuery)&#125;func parseQuery(RawQuery string) map[string]string &#123; parts := strings.Split(RawQuery, &quot;&amp;&quot;) queries := make(map[string]string, len(parts)) for _, part := range parts &#123; index := strings.IndexByte(part, &#x27;=&#x27;) if index == -1 || index == len(part)-1 &#123; continue &#125; queries[strings.TrimSpace(part[:index])] = strings.TrimSpace(part[index+1:]) &#125; return queries&#125; 先以&amp;符为分隔得到一个个k-v对，然后以&#x3D;符为分割分别得到key以及value，存入map即可。 7.readHeader()readHeader实现很简单，一直调用readLine读取一行，如果碰到CR+LF(\\r\\n\\r\\n)，这时readLine读取到的该行长度为0，也即代表首部字段的结束。我们将读到的每一行都保存到header这个map中，代码如下： 123456789101112131415161718192021222324func readHeader(bufr *bufio.Reader) (Header, error) &#123; header := make(Header) for &#123; line, err := readLine(bufr) if err != nil &#123; return nil, err &#125; //如果读到/r/n/r/n，代表报文首部的结束 if len(line) == 0 &#123; break &#125; //example: Connection: keep-alive i := bytes.IndexByte(line, &#x27;:&#x27;) if i == -1 &#123; return nil, errors.New(&quot;unsupported protocol&quot;) &#125; if i == len(line)-1 &#123; continue &#125; k, v := string(line[:i]), strings.TrimSpace(string(line[i+1:])) header[k] = append(header[k], v) &#125; return header, nil&#125; 8.setupBody()Body的设置是本框架的重难点之一，它是一个提供给用户的读取报文主体的接口，其将涉及到报文主体的解析，将是下一章的内容。我们这里暂时将Body设置成eofReader，让用户目前在Handler中无法从Body中读取任何数据。此处读者可以暂时略过。 12345678910type eofReader struct &#123;&#125;// 实现了io.Reader接口func (er *eofReader)Read([]byte)(n int,err error)&#123; return 0,io.EOF&#125;func (r *Request)setupBody()&#123; r.Body=new(eofReader)&#125; 9.绑定方法Request结构中的cookie以及queryString字段都是私有属性，因为只希望用户具有查询的权限，而不能够进行删除或者修改。为了让用户去查询这个私有字段，需要绑定相应的公共方法，这就是封装的思想。 除了安全性之外，利用公有方法的方式也能让我们的控制更加灵活，可以实现懒加载(lazy load)，从而提升性能。 比如在gin框架中每一个gin.Context中都会有一个叫做keys的map，用于在HandlerChain中传输数据，用户可以调用Set方法存数据，Get方法取数据。显而易见，为了实现这个功能，我们需要在操作keys这个map之前，就为其make分配内存。问题就出现在，如果我在生成一个gin.Context之初就为这个map进行初始化，但如果用户的Handler中并未使用这个功能怎么办？这个为keys初始化的时间是不是白白浪费了？ 所以gin采用了比较高明的方式，在用户使用Set方法时，Set方法会先检测keys这个map是否为nil，如果为nil，这时我们才为其初始化。这样懒加载就能减少一些不必要的开销。 我们给域名生成的cookie，一旦颁发给用户浏览器之后，浏览器在访问我们域名下的后端接口时都会在请求报文中将这个cookie带上，要是后端接口不关心客户端的cookie，而框架无脑全部提前解析，这就做了徒工。 所以也需要将Cookie的解析滞后，不是在readRequest中解析，而是在用户接口有需求，调用Cookie方法第一次查询时再进行解析。这就是为什么readRequest中没有解析cookie代码的原因。 接下来为Request绑定两个公有方法Query以及Cookie，分别用于查询queryString以及cookie： 123456789101112131415161718192021222324252627282930313233343536373839// 查询queryStringfunc (r *Request) Query(name string) string &#123; return r.queryString[name]&#125;// 查询cookiefunc (r *Request) Cookie(name string) string &#123; if r.cookies == nil &#123; //将cookie的解析滞后到第一次cookie查询处 r.parseCookies() &#125; return r.cookies[name]&#125;func (r *Request) parseCookies() &#123; if r.cookies != nil &#123; return &#125; r.cookies = make(map[string]string) rawCookies, ok := r.Header[&quot;Cookie&quot;] if !ok &#123; return &#125; for _, line := range rawCookies &#123; //example(line): uuid=12314753; tid=1BDB9E9; HOME=1(见上述的http请求报文) kvs := strings.Split(strings.TrimSpace(line), &quot;;&quot;) if len(kvs) == 1 &amp;&amp; kvs[0] == &quot;&quot; &#123; continue &#125; for i := 0; i &lt; len(kvs); i++ &#123; //example(kvs[i]): uuid=12314753 index := strings.IndexByte(kvs[i], &#x27;=&#x27;) if index == -1 &#123; continue &#125; r.cookies[strings.TrimSpace(kvs[i][:index])] = strings.TrimSpace(kvs[i][index+1:]) &#125; &#125; return&#125; 解析很简单，就是字符串的处理。 下面再啰嗦一下关于cookie。 首先解释一下，当客户端向服务器请求过某个资源后，服务器会向这个客户端颁发一个cookie。下次客户端再向服务器请求这个资源时，在请求报文中会携带这个cookie。此时服务器端只需要解析这个cookie便可以快速拿到资源返回给客户端。 先理解一下解析cookie是什么意思？ 解析cookie 的意思是有一个请求报文过来，这个请求报文会对应一个request结构体对象，我们需要根据这个请求报文里面的信息来给request结构体对象的各个属性赋值。其中有一步就会将请求报文里面的cookie赋值给request对象的cookie属性。这个任务就叫做解析cookie。具体的解析方法就是一次请求报文过来时，报文里面会带有cookies字段，cookies属于首部字段。而所有的首部字段我们都存进header结构体中了，所以我们要做的就是将cookies从header中取出来。然后赋值给这个请求报文对应的requset结构体对象里面的cookies属性。 所以我们如果把解析cookie的任务放在readRequest方法中，那么不管此时这个请求报文对应的request对象的cookie属性是否为空，他都会全部解析一遍再赋值给cookie属性。其实有的请求报文不是第一次来了，它的cookie早就被解析过了，已经被赋值过，所以不需要重复的解析。故不能放在此处。 那放在哪里呢？标准库中给Request结构体绑定了两个公用方法，一个是查询&amp;？&#x3D;的，一个就是查询cookie的。这个查询方法什么时候被调用呢？当客户端有请求过来时，框架会调用查询方法，查询此时这个请求报文对应的request结构体对象中的那个cookie属性是否为空？如果为空，就从这个请求报文中解析出cookie来补上这个属性，让这个请求对象的cookie属性不为空。 注意：当请求同一个资源时，第一次请求，请求报文第一次过来时，不带有cookie，所以此时request结构体中cookie属性为空。第二次请求，即第二次这个请求报文过来时，带有cookie了，所以此时应当给这个请求报文对应的request对象补上这个cookie属性。 所以，我们将解析cookie的任务放在查询cookie的方法中，当此时请求报文对应的request对象的cookie属性为空，我们就解析请求报文中的cookie给对象。不为空时，我们就不解析，直接返回这个cookie就行了。 10.测试为了方便我们的测试，我们为response.go进行如下更改： 123456789101112131415type response struct&#123; c *conn&#125;type ResponseWriter interface&#123; Write([]byte)(n int,err error)&#125;func setupResponse(c *conn)*response&#123; return &amp;response&#123;c:c&#125;&#125;func (w *response) Write(p []byte)(int,error)&#123; return w.c.bufw.Write(p)&#125; 为ResponseWriter加入Write方法，同时让response实现这个接口。这样我们可以在Handler中可以使用Write方法与客户端交互，从而手动构造http响应报文。 同时记得要在conn.go文件中对setupResponse进行调用： 123func (c *conn) setupResponse()*response&#123; return setupResponse(c)&#125; 测试文件main.go如下： 123456789101112131415161718192021222324252627282930313233343536373839package mainimport ( &quot;bytes&quot; &quot;example/httpd&quot; &quot;fmt&quot; &quot;io&quot;)type myHandler struct &#123;&#125;func (*myHandler) ServeHTTP(w httpd.ResponseWriter,r *httpd.Request)&#123; // 用户的头部信息保存到buff中 buff:=&amp;bytes.Buffer&#123;&#125; // 测试Request的解析 fmt.Fprintf(buff,&quot;[query]name=%s\\n&quot;,r.Query(&quot;name&quot;)) fmt.Fprintf(buff,&quot;[query]token=%s\\n&quot;,r.Query(&quot;token&quot;)) fmt.Fprintf(buff,&quot;[cookie]foo1=%s\\n&quot;,r.Cookie(&quot;foo1&quot;)) fmt.Fprintf(buff,&quot;[cookie]foo2=%s\\n&quot;,r.Cookie(&quot;foo2&quot;)) fmt.Fprintf(buff,&quot;[Header]User-Agent=%s\\n&quot;,r.Header.Get(&quot;User-Agent&quot;)) fmt.Fprintf(buff,&quot;[Header]Proto=%s\\n&quot;,r.Proto) fmt.Fprintf(buff,&quot;[Header]Method=%s\\n&quot;,r.Method) fmt.Fprintf(buff,&quot;[Addr]Addr=%s\\n&quot;,r.RemoteAddr) fmt.Fprintf(buff,&quot;[Request]%+v\\n&quot;,r) //手动发送响应报文 io.WriteString(w, &quot;HTTP/1.1 200 OK\\r\\n&quot;) io.WriteString(w, fmt.Sprintf(&quot;Content-Length: %d\\r\\n&quot;,buff.Len())) io.WriteString(w,&quot;\\r\\n&quot;) io.Copy(w,buff) //将buff缓存数据发送给客户端&#125;func main()&#123; svr:=&amp;httpd.Server&#123; Addr: &quot;127.0.0.1:8080&quot;, Handler: new(myHandler), &#125; panic(svr.ListenAndServe())&#125; 利用curl测试我们的接口： 12345678910111213141516# -b发送cookie，-i显示响应报文$ curl &quot;127.0.0.1:8080?name=gu&amp;token=123&quot; -b &quot;foo1=bar1;foo2=bar2;&quot; -i输出：HTTP/1.1 200 OKContent-Length: 488[query]name=gu[query]token=123[cookie]foo1=bar1[cookie]foo2=bar2[Header]User-Agent=curl/7.55.1[Header]Proto=HTTP/1.1[Header]Method=GET[Addr]Addr=127.0.0.1:4179[Request]&amp;&#123;Method:GET URL:/?name=gu&amp;token=123 Proto:HTTP/1.1 Header:map[Accept:[*/*] Cookie:[foo1=bar1;foo2=bar2;] Host:[127.0.0.1:8080] User-Agent:[curl/7.55.1]] Body:0x489290 RemoteAddr:127.0.0.1:4179 RequestURI:/?name=gu&amp;token=123 conn:0xc00007e690 cookies:map[foo1:bar1 foo2:bar2] queryString:map[name:gu token:123]&#125; 11.总结这一章我们对Request完成了请求行以及首部字段的解析，同时提供了更简便的查询queryString以及cookie的方法，我们已经完成了除POST以及PUT之外请求地解析。下一章我们将对Request的Body属性进行更为细致地设置，即对应POST请求的解析。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-（3）","slug":"从零实现Golang的HTTP标准库-（3）","date":"2022-10-06T00:20:00.000Z","updated":"2022-10-06T04:05:08.831Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（3）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%883%EF%BC%89.html","excerpt":"","text":"1.概述在正式开始HTTP协议的解析之前，我们先对几个包进行整理。 在上一篇中，我们对conn结构体进行了改进。 其中，提到了有关于标准库的bufio包的内容，包括bufio.Reader以及bufio.Writer等。 在上一篇中，对于这部分内容只是做了一个简单的介绍，在本篇中，我将单独对这部分内容进行一个梳理，并额外扩充了ioutil包以及io包等相关的内容。 这部分内容并不难，通常只是一些结构体和方法的梳理。但如果事先没能够很好理解结构体或方法的意义，往往会对理解后面的代码造成困难。 2.io包1.io包 IO 操作是我们在编程中不可避免会遇到的，例如读写文件，Go语言的 io 包中提供了相关的接口，定义了相应的规范，不同的数据类型可以根据规范去实现相应的方法，提供更加丰富的功能。 io.Reader io.Reader接口定义了 Read 方法，用于读取数据到字节数组中： 入参：字节数组 p，会将数据读入到 p 中 返回值：本次读取的字节数 n，以及遇到的错误 err io.Reader是一个接口，任何实现了Read()函数的对象，都可以作为Reader来使用 123type Reader interface &#123; Read(p []byte) (n int, err error)&#125; 方法功能详解 方法读取数据写入到字节数组 p 中，由于 p 是有大小的，所以一次至多读取 len(p) 个字节 方法返回读取的数据字节数 n(0 &lt;&#x3D; n &lt;&#x3D; len(p))，以及读取过程中遇到的 error 即使一次调用读取到的数据小于 len(p)，也可能会占用整个字节数组 p 作为暂存空间 如果数据源的数据量小于 len(p) 个字节，方法只会读取当前可用数据，不会等待更多数据的到来 io.Writer io.Writer接口定义了 Write 方法，用于写数据到底层文件中 入参：字节数组 p，会将 p 中的数据写入到底层文件中 返回值：成功写入完成的字节数 n，以及遇到的错误 err 123type Writer interface &#123; Write(p []byte) (n int, err error)&#125; 方法功能详解 该方法将 p 中的数据写到文件中 方法返回成功写入的字节数 n（0 &lt;&#x3D; n &lt;&#x3D; len(p)），以及写入过程中遇到的错误 err 如果 n&lt;len(p)，方法必须返回 err!&#x3D;nil 方法一定不能修改字节数组 p，即使是临时修改也不被允许 io.LimitedReader 它包含一个属性N代表能够在这个LimitReader上读取的最多字节数，如果在此LimitReader上读取的总字节数超过了上限，则接下来对这个LimitReader的读取都会返回io.EOF，从而有效终止读取过程，避免首部字段的无限读。 12345type LimitedReader struct &#123; R Reader // underlying reader N int64 // max bytes remaining&#125; io.copy(dst,src) 从src读,写入dst中.，复制文件 总结 io.Reader : 读取底层文件中的数据到字节数组p中io.Writer : 将字节数组p的数据写入到底层文件中 可以看到 Reader 和 Writer 接口中定义的方法中，都有字节数组p，而底层要操作的文件在方法中却没有体现出来,我们只需要知道底层文件会通过p被read和write操作即可。 Read方法是将文件的数据读入字节数组p，Write 是将字节数组p的数据写入文件，这一点不要记混。 3.ioutil包Go语言 ioutil包中提供了一些常用、方便的IO操作函数，我们在平时的时候中可以直接拿来使用。对于IO读操作来说，比较适用于读小文件，因为相关方法都是一次性将内容读入内存，文件太大内存吃不消。 ioutil.readAll（） readAll 是一个内部方法，从入参 io.reader 中读取全部数据，然后返回读取到的数据以及产生的 error，主要是调用 butes.Buffer 的 ReadFrom 方法（读取完数据产生的EOF error 在这里不算做 error，因为目的就是读取完数据）。 1234567891011121314151617181920212223242526272829// io.Reader r : 保存着底层数据，数据从 r 中读取// capacity： 用于设置保存数据的字节缓冲区的初始容量，但是在读取过程中会自动扩容的func readAll(r io.Reader, capacity int64) (b []byte, err error) &#123; var buf bytes.Buffer // 如果字节缓冲区在读取过程中一直扩容，最终超出了系统设置的最大容量，会产生 ErrTooLarge panic，在这里捕获，改为返回一个 error // 如果是其他类型的 panic，保持 panic defer func() &#123; e := recover() if e == nil &#123; return &#125; if panicErr, ok := e.(error); ok &amp;&amp; panicErr == bytes.ErrTooLarge &#123; err = panicErr &#125; else &#123; panic(e) &#125; &#125;() // 设置默认容量 if int64(int(capacity)) == capacity &#123; buf.Grow(int(capacity)) &#125; // 读取数据 _, err = buf.ReadFrom(r) return buf.Bytes(), err&#125; ioutil.Readfile（） ioutil.ReadFile 读取文件只需要传入一个文件名做为 参数，读取成功，会将文件的内容做为一个字节数组返回，如果读取错误，将返回 error 信息。 使用 ReadFile 读取文件，不需要手动 打开与关闭文件，打开与关闭文件的动作，系统自动帮我们完成。同时，使用 ReadFile 读取文件时，只适合读取小文件，不适合读取大文件。 总结 ReadAll 方法，我们比较常用的工具类方法，一次性读取文件的所有内容并返回，适用于读取小文件，如果文件太大会占用太多内存。调用 ReadAll 方法成功，会读取 io.Reader r 的所有内容，返回的 err &#x3D;&#x3D; nil，而不是 err &#x3D;&#x3D; EOF，因为读取完所有数据了，完成了我们的任务，此时 EOF 不应当是 error。 4.bufio包bufio.reader 我们都知道，对底层文件的IO操作，是比较费时的。如果每操作一次数据就要读取一下底层文件，IO操作是非常多的。那么如何提高效率呢？可以考虑预加载，读取数据的时候，提前加载部分数据到缓冲区中，如果缓冲区长度大于每次要操作的数据长度，这样就减少了 IO 次数；同样，对于写底入层文件，我们可以先将要写入的数据存入缓冲区，然后一次性将数据写入底层文件。 bufio包 基于缓冲区，提供了便捷的对底层文件IO操作方法，并利用缓冲区减少了IO次数，本篇文章就先来学习文件读取相关结构 bufio.Reader。 bufio.Reader 利用一个缓冲区，在底层文件读取器和读操作方法间架起了桥梁。底层文件读取器就是初始化 Reader 的时候需要传入的io.Reader。有这样一个缓冲区的好处是，每次我们想读取底层文件内容时，会首先从缓冲区读取，提高了读取速度，也避免了频繁的 底层文件IO，同时必要时会利用底层文件读取器io.Reader提前加载部分数据到缓冲区中，做到未雨绸缪。 有这样一个缓冲区的好处是，可以在大多数的时候降低读取方法的执行时间。虽然，读取方法有时还要负责填充缓冲区，但从总体来看，读取方法的平均执行时间一般都会因此有大幅度的缩短。 bufio.Reader 的结构如下： bufio.Reader结构 bufio.Reader中的 r、w 分别代表当前读取和写入的位置，读写都是针对缓存切片 buf 来说的。io.Reader rd 是用来写入数据到 buf 的，因此当写入了部分字节，w 会增大相应的写入字节数；而当从 buf 中读出数据后，r 会增大，被读取过的数据就是无用数据了。始终 w&gt;&#x3D;r，当 w&#x3D;&#x3D;r 时，说明写入的数据都被读取过了，没有有效数据可读了。 buf：用作缓冲区的字节切片，虽然是切片类型，但是一旦初始化完成之后，长度不会改变 rd：初始化时传入的io.Reader，用于读取底层文件数据，然后写入到缓冲区 buf 中 r：下一次读取缓冲区 buf 时的起始位置，即 r 之前的数据都是被读取过的，下次读取会从 r 位置开始，我们称之为已读计数 w：下一次写入缓冲区 buf 时的起始位置，即 w 之前都是之前写过的数据，下次写入从 w 位置开始，我们称之为已写计数 err：记录 rd 读取数据时产生的 error，err 在被读取或忽略之后，会被置为nil lastByte：保存上一次读取的最后一个字节的位置，用于回退一个字节；-1 表示无效值，不能回退 lastRuneSize：保存上一次读取的 rune 的位置，用于回退一个rune；-1 表示无效值，不能回退 12345678type Reader struct &#123; buf []byte rd io.Reader // reader provided by the client r, w int // buf read and write positions err error lastByte int // last byte read for UnreadByte; -1 means invalid lastRuneSize int // size of last rune read for UnreadRune; -1 means invalid&#125; Buffered() Buffered方法返回当前缓冲的字节数 1func (b *Reader) Buffered() int &#123; return b.w - b.r &#125; peek() peek方法用于查看未读数据的前n个字节，该方法并不会更改 bufio.Reader 的状态，不会更新已读计数，同时该方法不属于读取操作，不能用于后续的回退操作。 需要注意的是，该方法返回的是缓冲区的切片，可能造成数据泄露的风险，因为调用者可以通过返回的切片直接修改缓冲区的值；其次，返回数据的有效期是在下次数据读取之前，因为下次读取数据可能会数据压缩平移，导致当前数据的位置被改变。 1func (b *Reader) Peek(n int) ([]byte, error) Discard() Discard方法 会丢弃缓冲区的n个字节，最后返回实际丢弃的字节数和产生的 error。 对于合法参数 n，方法使用 for 循环不断装填数据，来尽量满足丢弃 n 个字节。即如果有效数据长度小于 n 的话，丢弃现有数据后，再重新调用fill 方法，填充新的数据用于丢弃，如果在这个过程中遇到err，方法就终止，最终返回实际丢弃的字节数和遇到的error。如果 buf 可丢弃的有效字节数大于 n，丢弃部分字节即可。 1func (b *Reader) Discard(n int) (discarded int, err error) Read() Read 方法读取数据到 字节切片 p 中，返回读取的字节数和产生的 error。 当缓冲区有效数据不为空时，直接将缓冲区的有效数据复制到字节切片p中，有多少就写入多少，不会再读取底层数据填充，因此如果当前缓冲区的有效数据长度小于传入字节切片 p 的长度，读取的字节数 n &lt; len(p)； 当缓冲区有效数据为空时，从底层文件读取数据，填充字节切片p。 当 p 的长度小于缓冲区长度时，从底层读取 一次 数据到缓冲区，然后将缓冲区的数据复制到 p 中 当 p 的长度大于缓冲区长度时，有一个优化，不会先写入缓冲区再复制到 p，这种方式不仅多复制一次，读取的数据还少于想要的数据长度，而是直接读取底层数据到 p 中，简单高效。 1func (b *Reader) Read(p []byte) (n int, err error) ReadLine() ReadLine方法 用于读取一行数据，且不会包含回车符和换行符(“\\r\\n” 或者 “\\n”)。该方法是 low-level 的，如果想要读取一行数据，应该尽量用 ReadBytes(‘\\n’) 或者 ReadString(‘\\n’) 来代替该方法。 在读取过程中，如果一行数据过长，超过了缓冲区长度，那么只会返回缓冲数组中的全部数据，并将 isPrefix 设置为 true，剩余的数据只会在后续再次调用 ReadLine方法 返回。如果正确返回一行数据，isPrefix&#x3D;false。 1func (b *Reader) ReadLine() (line []byte, isPrefix bool, err error) 总结 我们可以看到在bufio包下的read（）方法中只出现了字节切片p，也没有出现底层文件，这和io包下是一致的，我们只需知道底层文件确实被操作过了即可。 另外，bufio包的read机制需要熟记：首先切片p的含义是：要将底层文件的数据读取到p中当缓存切片bufr中有内容时，会优先从缓存切片中读内容到切片p中，并清空缓存切片bufr。当缓存切片bufr中没有内容时，会进行判断：当len(p) &gt; len(bufr)，即想要读取的内容比缓存切片bufr要大，直接去底层文件读取即可。当len(p) &lt; len(buf)，即想要读取的内容比缓存切片bufr小，先从底层文件读取内容充满缓存切片bufr，并将p填满，此时缓存切片bufr有剩余内容。再次读取时缓存切片bufr有内容，则将缓存区内容全部填入p并清空缓存切片bufr。 当缓存区中有内容时，程序的读取会从缓存区读而不会发生文件I&#x2F;O。只有当缓存区为空时才会发生文件I&#x2F;O 若缓存区的大小足够则启用缓存读，先会将内容载入填满缓存区，程序再从缓存区中读取。若缓存区过小则会直接从文件中读取而不使用缓存读 bufio.writer write机制：首先切片p的含义是：要将p中的数据写入到底层文件中当len(p) &gt; len(bufr)，即想要写入的内容比缓存切片bufr要大，直接往底层文件写入即可。当len(p) &lt; len(buf)，即想要写入的内容比缓存切片bufr小，先将p中的数据copy到缓存切片中去，再从缓存切片中将所有数据写入到底层文件中去。 flush（）方法 bufio.writer的flush方法，代表将缓存切片中剩余的数据一次性写入到底层文件中并清空缓存切片。 5.其他操作终端相关文件句柄常量： os.Stdin ： 标准输入 os.Stdout ： 标准输出 os.Stderr ： 标准错误 os.Open : 打开文件 os.OpenFile ：写入文件 WriteString()函数用于将字符串“s”的内容写入到写入器“w”, “w”是Writer类型的写入器，而“s”是写入写入器的字符串。 fmt.Printf，是把格式字符串输出到标准输出（一般是屏幕）。 1func Printf(format string, a ...interface&#123;&#125;) (n int, err error) fmt.Fprintf， 是把格式字符串输出到指定文件设备中，所以参数笔printf多一个文件指针FILE*。它返回已写入的字节数以及遇到的任何写入错误。 1func Fprintf(w io.Writer, format string, a ...interface&#123;&#125;) (n int, err error) 6.总结以上整理的基本上都是在接下来实现HTTP标准库过程中会用到的，暂时就整理这么多，其余的当我们遇到后再整理至此。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-（2）","slug":"从零实现Golang的HTTP标准库-（2）","date":"2022-10-05T14:20:00.000Z","updated":"2022-10-06T06:49:43.529Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（2）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%882%EF%BC%89.html","excerpt":"","text":"1.概述在上一篇中，我们介绍了如何启动一个Web服务， 在本篇中，我们主要构建框架的基本结构，即所需要的一些结构体。 我们暂时需要4个结构体： ① Server：代表WEB服务器，属性包含监听地址Addr以及Handler，负责服务器的启动逻辑。 ② conn：代表HTTP连接，net.Conn表达能力过弱，故将其封装成conn。仅服务于框架内部，不应由用户使用，包内不可导出。 ③ Request：代表客户端的HTTP请求，由框架从字节流中解析http报文从而生成的结构。 ④ response：代表响应，实现了ResponseWriter接口。包内不可导出。 2.结构体构建本节只为搭建框架的骨干，因此部分结构体内部暂时为空，部分函数也空实现，在随后的章节中我们一步步进行填充。 1.request.go 123type Request struct&#123;&#125;func readRequest(c *conn)(*Request,error)&#123;return nil,nil&#125; //暂时空实现 Request结构体就代表了客户端提交的http请求，我们使用readRequest函数从http连接上解析出这个对象。 它是http.Handler的ServeHTTP方法的第二个参数。 2.response.go 123type response struct&#123;&#125;type ResponseWriter interface&#123;&#125; response结构体就代表服务端的响应对象，我们后期会给其绑定些与客户端交互的方法，供用户使用。这里暂时让response和ResponseWriter都空实现。 它是http.Handler的ServeHTTP方法的第一个参数。 3.server.go 1234567891011121314151617181920212223type Handler interface &#123; ServeHTTP(w ResponseWriter,r *Request)&#125;type Server struct&#123; Addr string //监听地址 Handler Handler //处理http请求的回调函数&#125;func (s *Server) ListenAndServe()error&#123; l,err:=net.Listen(&quot;tcp&quot;,s.Addr) if err!=nil&#123; return err &#125; for&#123; rwc,err:=l.Accept() if err!=nil&#123; continue &#125; conn:=newConn(rwc,s) go conn.serve() // 为每一个连接开启一个go程 &#125;&#125; rwc是net包里的那个连接，此时还没有封装成代表http的连接。conn是封装好的连接对象。通过newConn（rwc）初始化出来。 conn是封装好的连接对象，它隶属于一个conn结构体。这个conn结构体里面提供了一个serve方法。这个serve方法就是用来处理连接的。Serve方法主要干三件事：一个是从连接中解析出客户端的请求request，另一个是设置response。这是serverHTTP方法的两个参数。最后一个是调用serverHTTP方法。注意：这个serverHTTP方法是和具体的tcp连接绑在一起的，而不是和封装好的那个连接绑在一起的。 上一节提到启动一个服务器其必须项只有Addr以及Handler，他们分别告诉了框架监听地址以及如何处理客户端的请求。事实上，Server结构体中还可以加入很多字段如读取或写入超时时间、能接受的最大报文大小等控制信息，但为了专注于一个框架最核心的实现，我们忽略这些细节内容。 ListenAndServe方法中展现的是go语言socket编程的写法，其大致意思是在Addr上监听TCP连接，将得到的TCP连接rwc(ReadWriteCloser)以及s进行封装得到conn结构体。接着调用conn.serve()方法，开启goroutine处理请求。 项目的开发尽量遵循模块分工原则，server.go只负责WEB服务器的启动逻辑，接下来的http协议的解析交给另一个模块conn.go进行。 4.conn.go 1234567891011121314151617181920212223242526272829303132333435type conn struct&#123; svr *Server // 引用服务器对象 rwc net.Conn // 底层tcp连接&#125;func newConn(rwc net.Conn,svr *Server)*conn&#123; return &amp;conn&#123;svr: svr, rwc: rwc&#125;&#125;func (c *conn) serve()&#123; defer func() &#123; if err:=recover();err!=nil&#123; log.Printf(&quot;panic recoverred,err:%v\\n&quot;,err) &#125; c.close() &#125;() //http1.1支持keep-alive长连接，所以一个连接中可能读出个请求，因此实用for循环读取 for&#123; req,err:=c.readRequest() //解析出Request if err!=nil&#123; handleErr(err,c) //将错误单独交给handleErr处理 return &#125; res:=c.setupResponse() //设置response // 有了用户关心的Request和response之后，传入用户提供的回调函数即可 c.svr.Handler.ServeHTTP(res,req) &#125;&#125;//暂时为空实现，后续小节再填充func (c *conn) readRequest()(*Request,error)&#123;return readRequest(c)&#125;func (c *conn) setupResponse()*response&#123;return nil&#125;func (c *conn) close()&#123;c.rwc.Close()&#125;func handleErr(err error,c *conn)&#123;fmt.Println(err)&#125; 对conn结构体作了扩充：拥有服务器属性、tcp连接属性。 在serve方法中，会分别调用readRequest以及setupResponse方法，从而得到Request以及response，随后将它们传入用户指定的Handler中，开启实际的请求处理过程。defer中使用recover，防止用户指定的Handler中存在逻辑错误导致发生panic。 利用for循环读取的原因： 对于HTTP 1.0来说，客户端为了获取服务端的每一个资源，都需要为每一个请求进行TCP连接的建立，因此每一个请求都需要等待2个RTT(三次握手+服务端的返回)的延时。而往往一个html网页中往往引用了多个css或者js文件，每一个请求都要经历TCP的三次握手，其带来的代价无疑是昂贵的。 因此在HTTP 1.1中进行了巨大的改进，即如果将要请求的资源在同一台服务器上，则我只需要建立一个TCP连接，所有的HTTP请求都通过这个连接传输，平均下来可以减少一半的传播时延。 如果客户端的请求头中包含connection: keep-alive字段，则我们的服务器应该有义务保证长连接的维持，并持续从中读取HTTP请求，因此这里我们使用for循环。 将err交给handleErr函数处理的原因： eadRequest可能会出现各种错误，如用户连接的断开、请求报文格式错误、服务器系统故障、使用了不支持的http版本、使用了不支持的协议等等错误。 对于有些错误如客户端连接断开或者使用了不支持的协议，我们服务端不应该进行回复。但对于一些错误如使用了不支持的http版本，我们应该返回505状态码；对于请求报文过大的错误，我们应该返回413状态码。因此在handleErr中，我们应该对err进行分类处理。 3.对conn结构体的改进目前conn结构体很简单，我们在读写两个方面进行分别改进，抽象出两个结构成员，一个负责对tcp连接读的逻辑，一个负责写的逻辑。两个改进如下： 1.写的改进 写的改进比较简单，主要是从性能优化角度出发。以下面代码为例： 12345http.HandleFunc(&quot;/&quot;, func(w http.ResponseWriter, r *http.Request) &#123; for i:=0;i&lt;100;i++&#123; io.WriteString(w,strconv.Itoa(i)) &#125; &#125;) 100次循环每次写入1~2B的小片段，每一次写入都会进行一次系统调用、一次IO操作，这势必会极大降低应用程序的性能。很显然，可以对用户写入数据进行缓存，缓存不下时再发送就能较少IO次数，从而提升效率。 go标准库提供了现有的工具，不需要重复造轮子。bufio.Writer可以解决我们的问题，它的底层会分配一个缓存切片，我们对这bufio.Writer写入时会优先往这个切片中写入，如果缓存满了，则将切片中缓存的数据发送到最底层的writer中，因此可以保证每次写入的大小都是大于或等于缓存切片的大小。在conn结构中引入bufw成员： 12345678910111213141516171819202122232425262728293031323334353637type conn struct&#123; svr *Server rwc net.Conn bufw *bufio.Writer //带缓存的writer&#125;func newConn(rwc net.Conn,svr *Server)*conn&#123; return &amp;conn&#123; svr: svr, rwc: rwc, bufw: bufio.NewWriterSize(rwc,4&lt;&lt;10), //缓存大小4kB &#125;&#125;func (c *conn) serve()&#123; defer func() &#123; if err:=recover();err!=nil&#123; log.Printf(&quot;panic recoverred,err:%v\\n&quot;,err) &#125; c.close() &#125;() //http1.1支持keep-alive长连接，所以一个连接中可能读出 //多个请求，因此实用for循环读取 for&#123; req,err:=c.readRequest() if err!=nil&#123; handleErr(err,c) return &#125; resp:=c.setupResponse() c.svr.Handler.ServeHTTP(resp,req) //将缓存中的剩余的数据发送到rwc中 if err=c.bufw.Flush();err!=nil&#123; return &#125; &#125;&#125; 我们给conn加入了bufw属性，以后的写入操作都将直接操纵bufw，其缓存的默认大小为4KB。同时在serve方法中，在一个请求处理结束后，bufw的缓存切片中还缓存有部分数据，我们需要调用Flush保证数据全部发送。 2.读的改进 对于HTTP协议来说，一个请求报文分为三部分：请求行、首部字段以及报文主体，一个post请求的报文如下： 123456789POST / HTTP/1.1\\r\\n #请求行Content-Type: text/plain\\r\\n #2~7行首部字段，首部字段为k-v对User-Agent: PostmanRuntime/7.28.0\\r\\nHost: 127.0.0.1:8080\\r\\nAccept-Encoding: gzip, deflate, br\\r\\nConnection: keep-alive\\r\\nContent-Length: 18\\r\\n\\r\\nhello,I am client! #报文主体 其中首部字段部分是由一个个key-value对组成，每一对之间通过\\r\\n分割，首部字段与报文主体之间则是利用两个连续的CRLF即\\r\\n\\r\\n作为分界。首部字段到底有多少个key-value对于服务端程序来说是无法预知的，因此我们想正确解析出所有的首部字段，我们必须一直解析到出现两个连续的\\r\\n为止。 对于一个正常的http请求报文，其首部字段总长度不会超过1MB，所以直接不加限制的读到空行完全可行，但问题是无法保证所有的客户端都没有恶意。 他可能在阅读框架源码后发现对首部字段的读取未采取任何限制措施，于是发送了一个首部字段无限长的http请求，导致服务器无限解析最终用掉了所有内存直至程序崩溃。因此我们应该为我们的reader限制最大读取量，这是第一个改进，改进用到了标准库的io.LimitedReader。 除此之外，首部字段的每个key-value都占用一行(\\r\\n是换行符)，为了方便解析，我们的reader应该有ReadLine方法。这是第二个改进，改进用到了标准库的bufio.Reader。 代码变动如下： 123456789101112131415161718type conn struct&#123; svr *Server rwc net.Conn lr *io.LimitedReader bufr *bufio.Reader //bufr是对lr的封装 bufw *bufio.Writer&#125;func newConn(rwc net.Conn,svr *Server)*conn&#123; lr:=&amp;io.LimitedReader&#123;R: rwc, N: 1&lt;&lt;20&#125; return &amp;conn&#123; svr: svr, rwc: rwc, bufw: bufio.NewWriterSize(rwc,4&lt;&lt;10), lr:lr, bufr: bufio.NewReaderSize(lr,4&lt;&lt;10), &#125;&#125; 第一处改进：为conn增加了lr字段，它是一个io.LimitedReader，它包含一个属性N代表能够在这个reader上读取的最多字节数，如果在此reader上读取的总字节数超过了上限，则接下来对这个reader的读取都会返回io.EOF，从而有效终止读取过程，避免首部字段的无限读。 第二处改进：为conn增加bufr字段，它是一个bufio.Reader，其底层的reader为上述的LimitedReader。对于一个io.Reader接口而言，它是无法提供ReadLine方法的，而将其封装程bufio.Reader后，就可以使用这个方法。 bufio.Reader相较io.Reader来说多出了ReadLine方法的原因： io.Reader提供的Read方法需要传入一个切片，如果传入的切片太小了，可能导致一行未读完；如果传入的切片太大了，则可能导致读取超过了一行。首部字段中的任何一行其长度是不可预知的，所以单纯利用io.Reader的Read方法很难达成目的。当然你可以传入一个字节大小的切片，每次读取1B然后通过不断append的方式，但这样会带来多次的IO开销。 bufio.Reader相较于io.Reader的改进就是，它会存在一个缓存切片，如果缓存切片中存在数据，我们对bufio.Reader进行Read优先会从这个缓存中取。我们平时会遇到一个使用场景就是，我们希望查看一下某个reader中的前多少B的数据，但又不希望我们这次查看之后后续的Read方法再也读不到这些数据，这时我们会将其转为一个bufio.Reader，通过其Peek方法就可以实现上述的要求。其原理就是Peek方法会将你peek出的数据暂存入切片缓存，尽管底层的reader流中不存在了这些数据，但对bufio.Reader进行Read会优先从缓存取，依旧可以将以前消费的数据读取出来。 ReadLine方法就是借助了这个缓存，它会不断地读取数据，如果读取的数据不够一行，则会将这些数据暂存；如果读取的数据够了一行，则将这一行返回，并将剩余未够一行的数据继续缓存。这样不论是一次读多读少，都不会影响Read方法的调用，同时也能减少IO次数提升性能。具体实现可以查看标准库bufio.go源码。 那么以后，我们直接操作的IO对象就是bufr和bufw： 读数据时直接操作bufr，bufr进而读取io.LimitedReader，进而读取tcp连接。 写数据时直接操作bufw，bufw进而写入到tcp连接。 4.测试编写main.go： 1234567891011121314151617181920package mainimport ( &quot;example/httpd&quot; &quot;fmt&quot;)type myHandler struct &#123;&#125;func (*myHandler) ServeHTTP(w httpd.ResponseWriter,r *httpd.Request)&#123; fmt.Println(&quot;hello world&quot;)&#125;func main()&#123; svr:=httpd.Server&#123; Addr: &quot;127.0.0.1:8080&quot;, Handler:new(myHandler), &#125; panic(svr.ListenAndServe())&#125; 由于目前未解析request以及response，所以无法去真正写我们的业务代码，但可以测试我们的Handler是否能被正常触发。执行curl命令： 1curl 127.0.0.1:8080 测试的执行流程为：框架使用者先指定一个myhandler结构体，给结构体绑定一个serverhttp方法，自己自定义实现serverhttp（其实是继承handler并重写）。 然后在main函数里面初始化一个server服务器对象svr，然后调用服务器对象的listenandserve方法，然后转进入到conn的serve方法，serve方法最后调用服务器对象的handler属性，handler是一个接口，里面有serverhttp方法但没有去实现，故handler可以调用serverhttp方法。 5.总结这一章我们完成了httpd框架骨干的搭建，以及完成对conn的封装。下一章则正式开始HTTP协议的解析工作，我们将封装Request、完成请求行以及请求首部字段的解析。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-（1）","slug":"从零实现Golang的HTTP标准库-（1）","date":"2022-10-05T02:21:31.000Z","updated":"2022-10-05T14:18:52.922Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-（1）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%EF%BC%881%EF%BC%89.html","excerpt":"","text":"1.前言在正式开始我们对Golang的HTTP标准库的实现之前，我还是想先推荐几个学习资源！ 这几个学习资源不是我们实现HTTP标准库所需的必备知识，所以我没有将他们放在前文的预备知识篇中，但是这几个资料的实现过程都和HTTP标准库有着一定相同的地方。 但如果你能够提前对以下推荐的几个学习资源事先做了阅读与研究，那么在实现Golang的HTTP标准库的过程中一定会如鱼得水，得心应手。 B站韩顺平的网络编程部分p294-p343 这一部分韩老师讲解了一个案例，其中的服务端部分思路值得借鉴。 B站刘丹冰的Zinx轻量级TCP服务框架 这个课程的质量非常高，up主刘丹冰老师的讲课风格十分干练，直击重点。在这门课中，老师实现了一个TCP服务框架，其实非常类似于我们将要实现的HTTP标准库，有很多思路都是共通的。我强烈建议大家有空的时候先完成对这门课程的学习，一定能够在这门课程中提前学到HTTP标准库的一些难点。 一位华科同学的博客 这个博客的博主是华中科技大学的一名计算机专业研究生，他在博客中记录了自己实现HTTP标准库的过程，其质量之高，让我收获良多。但由于博主计算机基础非常扎实，其语言风格比较干练，默认读者具有了一定的网络编程水平，故我在阅读的过程中对于一些细节之处难以理解，遇到了不少的阻碍。经过反复阅读与研究，我在阅读过程中做了不少的笔记，都是基于原博客写出来的，可以说，正是看了原博主的关于从零实现HTTP标准库的内容，让我萌发了对HTTP标准库研究的兴趣。所以，无论你之前是否有过网络编程基础，只要你想尝试研究实现HTTP标准库，你都应该或者说必须去看看这个博客。对于基础好的同学，我相信原博客内容已经能够满足你的需要；对于一些和我一样事先不太了解网络底层编程的同学，我觉得你仍需先看看原文的博客，当看到某处难以理解时，可以参考参考我博客中对应的地方，也许这处也是我当时疑惑的点。因此强烈建议各位去看看这个博客，我在此也对博主辜飞俊同学表示感谢！ 极客时间web框架教程 这是一门手把手教你写一个web框架的课程，总体来说应该具有一定的难度。我目前也没有学习完这门课程，但我认为其质量也是非常好的，可以作为我们实现完HTTP框架后的一个进阶与补充。其中，文章开头就以Golang的net&#x2F;HTTP标准库为例，引领读者一步步实现web框架。 以上四个内容都是我在学习过程中发现的优质资源，都对我有着比较大的帮助，所以也分享出来给大家学习。 2.概述Go语言的官方net&#x2F;HTTP标准库，搭建一个webServer非常容易。因为这个搭建过程中所使用到的函数、方法、接口往往是标准库帮我们封装好了的，我们只需要根据现成的已有的工具即可完成搭建。而我们今天将要实现的自然并不是使用现成的工具，而是要钻进HTTP标准库源码中，看看这些工具是怎么设计、怎么封装的。 3.webServerwebServer在维基百科上的解释为：Web Server 是一个通过 HTTP 协议处理 Web 请求的计算机系统。 HTTP 协议，在 OSI 网络体系结构中，是基于 TCP&#x2F;IP 之上第七层应用层的协议，全称叫做超文本传输协议。啥意思？就是说 HTTP 协议传输的都是文本字符，只是这些字符是有规则排列的。这些字符的排列规则，就是一种约定，也就是协议。这个协议还有一个专门的描述文档，就是RFC 2616。 对于 HTTP 协议，无论是请求还是响应，传输的消息体都可以分为两个部分：HTTP 头部和 HTTP Body 体。头部描述的一般是和业务无关但与传输相关的信息，比如请求地址、编码格式、缓存时长等；Body 里面主要描述的是与业务相关的信息。 Web Server 的本质，实际上就是接收、解析 HTTP 请求传输的文本字符，理解这些文本字符的指令，然后进行计算，再将返回值组织成 HTTP 响应的文本字符，通过 TCP 网络传输回去。 4.标准库启动Web服务的示例用 net&#x2F;http 来创建一个 HTTP 服务，其实很简单，下面是官方文档里的例子。我做了些注释，帮你理解。 1234567891011// 创建一个Foo路由和处理函数http.Handle(&quot;/foo&quot;, fooHandler)// 创建一个bar路由和处理函数http.HandleFunc(&quot;/bar&quot;, func(w http.ResponseWriter, r *http.Request) &#123; fmt.Fprintf(w, &quot;Hello, %q&quot;, html.EscapeString(r.URL.Path))&#125;)// 监听8080端口log.Fatal(http.ListenAndServe(&quot;:8080&quot;, nil)) 有基础的同学是可以看得懂上述代码的，无非是注册了两个路由，然后让服务器监听在8080端口上并运行。 但是零基础的同学看着上述代码，那可就疑问多多了，Handle是啥？HandleFunc是啥？ListenAndServe又是啥？有什么作用呢？不要担心，我最开始也是这样。 不要在意上面的代码细节，暂时有个印象，先继续往下看吧！ 5.自编代码启动Web服务第4节中的代码是官方的写法，他其实已经是经过了一些封装和简化的，那我们现在就自己动手写一个启动Web服务的代码，没有封装、没有简化，从这个自编代码入手开始学习！ 注意：官方代码中前两个方法是注册路由，关于这一块我们目前无需涉及，所以我先将路由这部分省略！ 1234567891011121314151617package mainimport &quot;net/http&quot;type myHandler struct&#123;&#125;func (*myHandler) ServeHTTP(w http.ResponseWriter,r *http.Request)&#123; w.Write([]byte(&quot;hello world!&quot;))&#125;func main()&#123; svr:=&amp;http.Server&#123; Addr: &quot;127.0.0.1:8080&quot;, Handler: new(myHandler), &#125; panic(svr.ListenAndServe())&#125; 我们来解析一下上述代码！解析过程并没有按照代码顺序，因为代码间存在先后因果的逻辑关系，需要仔细梳理！ 第1-3行为在main包下，引入“net&#x2F;http”库。 在main函数中，第12行我们初始化了一个http.server的对象并对其进行属性进行赋值，起名叫svr。 那么这个http.server是什么呢？它其实是一个结构体，代表了一个我们将要启动的服务器。 123456789type Server struct &#123; // 请求监听地址 Addr string // 请求核心处理函数 Handler Handler // 为了理解方便，其余非核心属性省略 ...&#125; 你可以看到这个server服务器结构体中有两个核心属性： 一个是String类型的Addr，初始化server时给Addr赋值表示希望服务器监听哪个地址端口； 一个是Handler类型的Handler，Handler是一个有ServeHTTP方法的接口，就好比一个拦截所有http请求的拦截器，告诉框架如何处理来自客户端的所有http请求。 我们来重点解析一下这个Handler接口。它代表了一个抽象的业务处理逻辑。 在自编代码中，我们在初始化服务器对象时，除了给Addr赋值，我们也给Handler属性进行了赋值，其值为myHandler。 显而易见，这个myHandler是我们自己定义的一个结构体，它会去实现Handler接口，因而myHandler可以赋值给Handler。前面说了这个Handler代表了一个抽象的业务处理逻辑，我们自己写这个myHandler并赋值给Handler，目的就是我们想要自己定义这个处理逻辑。 注意： 不论是Handler还是myHandler，他们都是代表抽象的笼统的，请不要将myHandler理解为具体的业务处理方法。具体的处理方法是另一个东西叫serverHTTP方法，这个serverHTTP方法是Handler接口里的一个成员方法，而myHandler结构体实现了Handler里的这个成员方法，也即实现了Handler接口。 在初始化server时，也可以不给Handler进行赋值，也即我们不想要自定义处理方法，那么这时候标准库会启动自己的默认处理方式。 在serverHTTP方法中我们才真正的去定义具体的业务请求处理方法。 比如在自编代码中，我们对业务请求的处理非常简单：直接输出“hello world!” 关于这个serverHTTP方法的参数： w http.ResponseWriter ：从名字你可以将它理解为“响应构造器”。当我们收到一个HTTP请求后，我们的框架会对这个请求进行处理。请求处理完毕后，肯定要往回发送一个响应。我们直接调用这个响应构造器w的write方法，即可完成响应的发送。 r *http.Request ：这个很简单，就是我们服务器或者框架从客户端收到的请求。这个请求的类型为Request，很显然，这个类型到时候是需要我们后面自己构建的。 其实，当客户端发来一个请求时，它的请求形式是一个HTTP请求报文的形式。而我们服务器对客户端的响应也是一个HTTP响应报文的形式。我们框架要做的就是当接收到HTTP报文后，我们对HTTP报文进行解析，获取关键信息并将其封装成一个Request结构体形式，代表一个请求。有了这个请求后，进行处理请求，通过响应构造器的write方法构建一个HTTP响应报文，再将其发送回去。 最后，关于ListenAndServe()函数，可以参照一下下面这幅图。 这部分其实就和go的socket编程相关。 如果你觉得层次比较多，对照着思维导图多看几遍就顺畅了。这里我也给你整理了一下逻辑线各层的关键结论： 第一层，标准库创建 HTTP 服务是通过创建一个 Server 数据结构完成的； 第二层，Server 数据结构在 for 循环中不断监听每一个连接； 第三层，每个连接默认开启一个 Goroutine 为其服务； 第四、五层，serverHandler 结构代表请求对应的处理逻辑，并且通过这个结构进行具体业务逻辑处理； 第六层，Server 数据结构如果没有设置处理函数 Handler，默认使用 DefaultServerMux 处理请求； 第七层，DefaultServerMux 是使用 map 结构来存储和查找路由规则。 最后，再多说一句，这个Handler接口除了有自定义处理逻辑的功能外，还有着路由的功能。 Handler的存在给框架的拓展带来了极大的灵活性，有了Handler，我们可以让任何一个HTTP请求以自己的规则映射到自己的路由。比如http标准库用ServeMux类型实现了Handler接口，从而实现了静态路由(将在本系列的末尾讨论)；gin的gin.Engine也是实现了自己的Handler，有了动态路由功能。 路由部分我们最后再说。 6.自定义框架的需求分析从需求分析的角度出发，看看我们要实现的web框架大体上需要哪些功能： http协议的解析不应该由开发者完成，我们需要从tcp字节流中解析出http的报文。 框架需要设置Request并为Request绑定易用API。 框架需要设置Response并为Response绑定易用API。 乍一看，我们需要给框架完成的功能甚少，但每一步都会有很多情况需要处理： 比如对于http 1.1协议来说，因为支持长连接，一个tcp连接能发送多个http请求，如果框架未正确完成上一个请求的解析(如未将当前报文主体全部读完)，那么随之到来的下一个请求就无法正确解析。 客户端有时会以chunk方式传输报文主体，我们应该保证用户read到的只有有效载荷(playload)，而没有chunk协议里的控制信息。 前端提交上来的form表单有多种类型，最常见的如application&#x2F;x-www-form-urlencoded以及multipart&#x2F;form-data，我们框架应该予以区分并分别提供解析方法。 服务端发送的数据是放在http响应报文的响应体中，客户端怎么知道我们发送了多少数据呢？一般来说可以查看响应头中的Content-Length字段，从而知道响应体的长度。观察上述的代码的ServeHTTP方法，我们并没有显式为头部指定Content-Length，但客户端依旧可以完整的读取出数据，这就说明标准库帮助我们完成了相关的设置工作。 从可行性角度来说，框架为我们的每一次响应都自动正确设置Content-Length(以下简称CT)是不现实的，发送CT所在的响应头必须是先于发送响应报文主体的，如果框架要自动设置CT，也就意味着我们必须为用户Write的所有数据进行缓存，这对一定长度内的发送量还实用，但对于大响应主体来说绝对是不可行。所以我们的框架还需要在必要时刻转化为利用chunk方式传输数据，这一部分对用户来说必须是无感知的。 7.总结本篇是从零实现Golang的HTTP标准库服务端部分的第一篇，开头给大家推荐了几个优秀的学习资源，然后介绍了webServer，标准库启动webServer的方法以及我们自定义启动webServer的方法并进行了详细的分析，最后给出了我们框架的需求以及难点。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}]},{"title":"从零实现Golang的HTTP标准库-预备知识（2）","slug":"从零实现Golang的HTTP标准库-预备知识（2）","date":"2022-10-04T14:17:12.000Z","updated":"2022-10-04T10:30:26.038Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-预备知识（2）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%EF%BC%882%EF%BC%89.html","excerpt":"","text":"1.概述在预备知识（1）中，我们介绍了三个学习内容，其中前两个学习内容：go语言的基础语法以及计算机网络的概念了解，我们不再详细介绍与梳理，请读者自行完成这两个部分的学习。 在本篇中，我将对socket编程流程做一个简单的介绍，同时梳理一下go语言的socket编程。 本篇默认你已经对socket有所了解，故基本的概念性问题将不再被提及。如果你在阅读以下内容的过程中，有任何不明白的地方，去看上一篇文章中推荐的学习资源，里面应该都会有答案，或者直接网上搜索疑问。 2.传统TCP套接字编程流程注意：以下内容是我学习《计算机网络自顶向下》中相关部分自己总结而来，可能存在说辞严谨性问题，目的只是为了提供给大家一个借鉴和参考。具体也可以听一听中科大郑老师的课上的讲解！ TCP套接字编程流程：1 服务器首先运行，等待连接建立。 1.1 服务器创建一个欢迎socket，即可以返回一个整数。 此时这个整数无任何意义。创建socket可以调用socket API的创建函数。 1.2 将这个整数和服务器本地的IP和服务器本地端口相捆绑，捆绑可以调用socket API的捆绑函数。 1.3 在服务器的欢迎scoket上阻塞式的等待接收用户的连接。即调用socket API的accept函数，接收来自远端的用户和服务器的欢迎scoket进行TCP连接。如果此时没有连接，则函数停在当前不往下走，即阻塞式等待连接。 2 客户端主动与服务器建立连接 2.1 客户端创建本地socket，不需要捆绑，是隐式自动捆绑。 2.2 指定服务器进程的IP地址和端口号，与服务器进程连接。 3 服务器收到来自客户端的连接请求3.1 服务器接收来自用户端的连接请求，解除阻塞式等待。 此时服务器会返回一个新的socket整数 新的socket叫connection socket，这个connection socket仍然和服务器的IP、端口捆绑但同时connection socket又和客户端的IP和端口相捆绑。此时连接建立完成。 这个新的connection socket就是通道，在这个通道上就可以收和发。 3.传统TCP套接字编程伪代码1.welcomeSocket&#x3D;Socket（）;&#x2F;&#x2F;创建一个欢迎socket 2.bind（welcomeSocket&amp;sad）&#x2F;&#x2F;将欢迎socket和服务器本地IP、端口相捆绑 3.connectionSocket&#x3D;accept（welcomeSocket）&#x2F;&#x2F;在welcomeSocket所在的的端口上阻塞式等待来自客户端用户的连接请求 4.clientSocket&#x3D;socket（）；&#x2F;&#x2F;创建一个客户端本地的clientSocket 5.connect（clientSocket，sad）&#x2F;&#x2F;将客户端的socket和服务器的ip、端口捆绑此时客户端的TCP实体向服务端发出一个连接建立请求，服务端收到请求后解除阻塞，accept会返回一个新的值connectionSocket，且服务端TCP实体给出连接建立响应，客户端处connect返回一个有效值clientSocket，此时连接建立。 6.客户端用clientSocket发送信息，用到send函数。 7.服务器端用connectionSocket接收信息，用到read函数。 8.服务器端用connectionSocket处理信息，用到write函数。 9.客户端用clientSocket接收信息，用到read函数。 10.关闭掉本次connectionSocket，等待用户下一次连接建立请求。关闭掉本次clientSocket。 用到close函数 以上为传统编码形式的TCP Socket编程。 (1) 建立Socket：使用socket()函数。 (2) 绑定Socket：使用bind()函数。 (3) 监听：使用listen()函数。或者连接：使用connect()函数。 (4) 接受连接：使用accept()函数。 (5) 接收：使用receive()函数。或者发送：使用send()函数。 4.go的socket编程以下内容是我学习B站韩顺平老师go语言基础课中网络编程部分的笔记，具体代码可以去视频中参考。 Golang TCP Socket编程： 1.分为服务器端和客户端，客户端可以有多个。 2.服务器端在8888端口监听，等待客户端的连接。 3.客户端向服务器端IP地址的端口8888建立连接，这个连接的双向箭头是Conn类型的，先记住。 4.服务器端有一个主线程P，当某一个客户端与服务器端建立连接后，这个主线程会开辟一个单独的协程goroutine来处理这个客户端的请求。 5.若有多个客户端，则可以开辟多个协程来处理。 5.go语言net包中的主要函数介绍本部分主要介绍net包中一些以后会用到的函数方法，详细内容参考以下链接。 https://studygolang.com/pkgdoc 12package netimport &quot;net&quot; net包提供了可移植的网络I&#x2F;O接口，包括TCP&#x2F;IP、UDP、域名解析和Unix域socket。 虽然本包提供了对网络原语的访问，大部分使用者只需要Dial、Listen和Accept函数提供的基本接口；以及相关的Conn和Listener接口。 1.Listen函数 func Listen(net, laddr string) (Listener, error)，用Listener类型和error类型接收。 解析：Listen函数是net包直接调用的函数，写作net.Listen（）。它里面含有两个参数，其中net是指网络协议的类型：TCP,UDP等，laddr是一个网络地址，由IP：端口号组成。Listen函数的作用是返回在一个本地网络地址laddr上监听的Listener。 net.Listen（x1，x2）就代表让服务器守候在x2端口上等候客户端的连接请求，并且连接必须是x1协议类型的。 2.Listener接口 12345678type Listener interface &#123; // Addr返回该接口的网络终端地址 Addr() Addr // Accept等待并返回下一个连接到该接口的连接 Accept() (c Conn, err error) // Close关闭该接口，并使任何阻塞的Accept操作都会不再阻塞并返回错误。 Close() error &#125; 如上所示，Listenr是一个监听接口类型，它是Listen函数的返回值类型。 比如说listenr是Listen函数的返回值，listener是Listener接口类型的“对象”。由最初的TCP套接字编程流程可知，accept函数是阻塞式的等待客户端和服务器进行连接，若连接上了，返回一个connection socket。 若没有连接上，则继续等待。在这里我们就可以用listenr对象调用Accept方法来实现。即listener.Accept（），因为accept是接口里的方法，所以可以直接调用。又因为需要阻塞式等待，所以该语句应该写在一个无限循环中。listener代表一个监听接口对象。关闭listener代表不监听了。 Accept() (c Conn, err error)再来细看Accept方法，返回值是Conn类型和error类型。 2.Conn接口 1234567891011121314type Conn interface &#123; // Read从连接中读取数据 Read(b []byte) (n int, err error) // Write从连接中写入数据 Write(b []byte) (n int, err error) // Close方法关闭该连接 // 并会导致任何阻塞中的Read或Write方法不再阻塞并返回错误 Close() error // 返回本地网络地址 LocalAddr() Addr // 返回远端网络地址 RemoteAddr() Addr&#125; Conn是Accept函数的返回值类型。 Conn接口代表网络连接。多个线程可能会同时调用同一个Conn的方法。 Conn也是一个接口类型。 这个Conn类型代表了一个客户端与服务器端的连接，也即前面图中提到的双向箭头，就是Conn接口类型的。 假如设Conn接口的对象是conn，那么这个conn就是传统编码中服务器端返回的那个connection socket。我们可以对这个conn进行读数据操作和写数据操作。 6.总结以上内容是我当时在学习这些知识时自己做的一些笔记，仅供大家参考！ 当你差不多掌握了上述预备知识以后，下面我们将真正步入Golang的HTTP标准库的实现中去！ 对于上述内容，你可以在实践的过程中慢慢体会！ 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"从零实现Golang的HTTP标准库-预备知识（1）","slug":"从零实现Golang的HTTP标准库-预备知识（1）","date":"2022-09-25T14:09:40.000Z","updated":"2022-10-04T10:32:00.663Z","comments":true,"path":"/post/从零实现Golang的HTTP标准库-预备知识（1）.html","link":"","permalink":"http://example.com/post/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Golang%E7%9A%84HTTP%E6%A0%87%E5%87%86%E5%BA%93-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%EF%BC%881%EF%BC%89.html","excerpt":"","text":"1.概述在我们从零实现Golang的HTTP标准库之前，我们首先需要学习一些预备知识，学习了这些预备知识作为基础以后，我们才能更好的理解Golang的HTTTP标准库的代码，也有利于我们后面自己动手实现它。 在预备知识中，我不会过多介绍关于Golang的HTTP标准库有关的内容，这一部分的介绍等我们学习完预备知识以后，会更新在后续的真正实现HTTP标准库的文章中。 2.所需要的预备知识以及学习资源推荐 Golang的基本语法 B站韩顺平的go语言基础课 《go语言编程基础》 国内七牛云团队编写的书，国人编写，通俗易懂 《go语言程序设计》 Go语言圣经，经典书籍但翻译略显晦涩 因为我们是要实现Golang的HTTP标准库，所以首先我们需要了解Golang这门语言的基础语法。 当然，我们不需要对Go语言有特别深入的了解，只需要掌握一些基础的部分，如：变量的定义、结构体的创建、给结构体绑定函数、接口的定义、接口的实现、for循环的写法、结构体对象的初始化等等，以上这些经常出现在标准库代码中。 至于Go语言的一些深入特性，如闭包、管道、Groutine协程等的底层原理，在本系列中无需知晓，只需知道其作用即可。 计算机网络 B站中科大计算机网络教程 《计算机网络自顶向下》 经典书籍，你只需学习书中的应用层和传输层即可，在传输层的结尾你会了解到传统socket的编程方式。 这门课是计算机科学与技术专业或软件工程专业学生的必修专业课，要想知道什么是HTTP，自然离不开计算机网络的学习。 你需要学习有关于传输层的知识，包括TCP等，以及什么是socket。力求对于计算机网络课程有一个大体的概念了解。 go语言的socket编程 B站韩顺平的go语言基础课网络编程部分 在计算机网络课程中，你会了解到什么是socket，以及传统的socket编程方式流程步骤。 但是在go语言中，有着自己的一套socket编程方法，但是大体的思路原理却是一致的，所以你仍需先学习计算机网络，了解传统的socket编程，在此基础上，学习go语言的socket编程将会事半功倍。 注意：我们需要学习的是有关于TCP的socket编程。 3.总结关于上述的三个知识点或者说三部分学习内容，难度不大，不需要花费你太多的精力。 但是却是我们实现Golang的HTTP标准库的基础，所以需要认真学习对待。 在接下来的文章中，我也会将其中一些重要部分单独梳理一遍。 本篇到此结束，感谢你的阅读！","categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"}],"tags":[{"name":"基础知识","slug":"基础知识","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}]},{"title":"小戴小戴","slug":"小戴小戴","date":"2022-09-25T06:19:54.000Z","updated":"2022-10-05T14:19:59.553Z","comments":true,"path":"/post/小戴小戴.html","link":"","permalink":"http://example.com/post/%E5%B0%8F%E6%88%B4%E5%B0%8F%E6%88%B4.html","excerpt":"","text":"小戴小戴，这里是小徐。","categories":[],"tags":[]},{"title":"Markdown语法学习，博客记录的开始~","slug":"markdown语法","date":"2022-09-24T15:08:35.025Z","updated":"2022-10-05T14:19:47.371Z","comments":true,"path":"/post/markdown语法.html","link":"","permalink":"http://example.com/post/markdown%E8%AF%AD%E6%B3%95.html","excerpt":"","text":"1.标题一共会有6个等级的标题，用#号表示，#数量越多，越小，一般情况下不要使用第一级标题，从二级标题开始用。 titlel1 我是一级titlel2 我是二级titlel3 我是三级titlel4 我是四级titlel5 我是五级titlel6 我是六级2.段落想要分段，在段尾输入至少两个以上的空格再回车；或者直接空一行再输入。 如：祝爷爷奶奶身体健康 祝我的家人生活幸福 这两句想要分段 就在第一句“祝爷爷奶奶身体健康”结束后空一整行再输入第二句“祝我的家人生活幸福”即可得到： 祝爷爷奶奶身体健康 祝我的家人生活幸福 3.字体3.1字体加粗：如想对“同济大学”加粗，在“同济大学”两侧分别加上两个“*”号，即可实现加粗。 “同济大学” 3.2字体倾斜：如想对“同济大学”倾斜，在“同济大学”两侧分别加上一个“*”号，即可实现倾斜。 “同济大学“ 3.3高亮：如想对“同济大学”高亮，在“同济大学”左边加上”&lt;mark&gt;”，右边加上”&lt;&#x2F;mark&gt;”,即可实现倾斜。 “同济大学“ 4.分隔线当文章段落过多，会影响整体可读性，此时可用分隔线对文章进行切分。 直接在单独的一行中输入三个“*”号。 或者使用多个“-”。 5.删除线在要添加删除线的文本两侧分别加上两个“~”。 文本文本文本 6.无序列表无序列表就是在输出多行文字，每行最前面带一个小点作为前缀。语法为一个“*”跟一个空格，后面输入文字，每行之间不需要空一行。 xxxxxxxxx yyyyyyyyy zzzzzzzzzz 7.有序列表有序列表就是用数字作为前缀。语法为一个数字带一个点跟一个空格，后面输入文字，每行之间不需要空一行。 xxxxxxxxxxxx yyyyyyyyyyyy zzzzzzzzzzzzz 8.列表嵌套在输入完第一个列表行后，想在这个列表行下在嵌套两个子列表行，则在第一个列表行的下一行开始连按四个空格，再逐个输入子列表行。 列表行A 子列表1 子列表2 列表行B 子列表1 子列表2 子列表3 9.区块区块是用来引用一段文本的。 语法为：一个”&gt;”跟一个空格，后面输入需要引用的文本。 如果要分段，则在第二行输入一个单独的”&gt;”，再换到第三行按照语法继续引用。 扬州是个好地方啊。 —-习近平 把扬州建设成为古代文化与现代文明交相辉映的名城。 —-江泽民 10.代码块代码块用来展示不同语言的代码。 语法为：上下分别使用三个点进行包裹，上下各占一行，在顶行的三个点右边写上代码语言的类型(java,go)从而让代码高亮，无需空格。 其中点为esc键下面的那个波浪键，注意一定要用英文的点。 123456public class Hello&#123; public static void main(String[] args)&#123; System.out.print(&quot;Hello,World!!&quot;); &#125;&#125; 1234567package mainimport ( &quot;fmt&quot;)func main() &#123; fmt.Println(&quot;Hello,World!&quot;)&#125; 11.链接 直接插入 直接显示链接的网址，也即直接复制连接过来，点击网址即可跳转。 如 www.baidu.com 有时候无法识别网址，所以在插入链接的时候最好用“&lt;&gt;”包围来插入，这样百分百可识别为网址 文本链接 就是让一段文本具有链接跳转的功能。 语法为：英文下，一对方括号“[]”加上一对圆括号“（）”，在方括号内写文本，在圆括号内写链接网址，点击文本即可跳转,网址需要写全，前缀加上。 如：百度一下 12.图片直接将要插入的图片拖入图床，用markdown格式在vika模式下上传，然后直接粘贴就行了。 文字居中：不能直接按空格让文字居中，而是在文字左侧加”&lt;center&gt;”,右侧加”&lt;&#x2F;center&gt;”。 图片居中：在图片链接结尾的右括号前面加上”#pic_center”。 扮鬼脸的哆啦A梦！ 图1 13.表格 x y z a c e b d f 利用多个”|”，每两个“|”之间写内容。 在表格的第二行中，每两个“|”之间填的是“—”，减号数量不影响显示，这个不会显示出来，只是用来确定格式。 第二行和第一行保持一致，用“—”代替。在“—”的左右两边都可以添加冒号“:”,在左边添就是居左对齐，在右边添就是居右对齐，两边都添就是居中对齐。 x y z a c e b d f 14.转义字符如果我们想输出某些符号，但是在markdown里面有可能被识别成语法，此时需要在前面添上转义字符。 通常用反斜杠”&quot;直接加上符号，无需空格。 反斜杠英文下直接按enter键上面那个，不用按shift。 * 666 这是加了反斜杠的星号。 666 这是没加反斜杠的星号，被识别成无序列表前缀。 # ​","categories":[{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/categories/Markdown/"}],"tags":[{"name":"Markdown语法基础","slug":"Markdown语法基础","permalink":"http://example.com/tags/Markdown%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/"}]}],"categories":[{"name":"从零实现HTTP标准库","slug":"从零实现HTTP标准库","permalink":"http://example.com/categories/%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0HTTP%E6%A0%87%E5%87%86%E5%BA%93/"},{"name":"Markdown","slug":"Markdown","permalink":"http://example.com/categories/Markdown/"}],"tags":[{"name":"网络编程","slug":"网络编程","permalink":"http://example.com/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"基础知识","slug":"基础知识","permalink":"http://example.com/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"Markdown语法基础","slug":"Markdown语法基础","permalink":"http://example.com/tags/Markdown%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/"}]}